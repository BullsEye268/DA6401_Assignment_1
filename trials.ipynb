{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: bullseye2608 (bullseye2608-indian-institute-of-technology-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(X, Y, val_ratio=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Splits X and Y into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data of shape (N, d).\n",
    "    - Y: Corresponding labels of shape (N, ...).\n",
    "    - val_ratio: Fraction of data to be used for validation (default is 0.2).\n",
    "    - seed: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_val, Y_train, Y_val: The split datasets.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    # Generate a random permutation of indices\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    # Determine split index based on the validation ratio\n",
    "    split_index = int(n_samples * (1 - val_ratio))\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    X_val = X[val_indices]\n",
    "    Y_val = Y[val_indices]\n",
    "    \n",
    "    return X_train, X_val, Y_train, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "(X_train, X_val, y_train, y_val) = create_validation_set(X_train, y_train, val_ratio=0.1, seed=42)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Create a DataFrame for the training data\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "train_df = pd.DataFrame(X_train_flat)\n",
    "train_df['label'] = y_train\n",
    "train_df['label_name'] = [class_names[label] for label in y_train]\n",
    "\n",
    "# Create a DataFrame for the validation data\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "val_df = pd.DataFrame(X_val_flat)\n",
    "val_df['label'] = y_val\n",
    "val_df['label_name'] = [class_names[label] for label in y_val]\n",
    "\n",
    "# Create a DataFrame for the test data\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "test_df = pd.DataFrame(X_test_flat)\n",
    "test_df['label'] = y_test\n",
    "test_df['label_name'] = [class_names[label] for label in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DELL\\Desktop\\Coding\\Python\\DL\\wandb\\run-20250227_220535-jr0mzoz8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/my-awesome-project/runs/jr0mzoz8' target=\"_blank\">Images_2025-02-27 22:05:35</a></strong> to <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/my-awesome-project' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/my-awesome-project/runs/jr0mzoz8' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/my-awesome-project/runs/jr0mzoz8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Images_2025-02-27 22:05:35</strong> at: <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/my-awesome-project/runs/jr0mzoz8' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/my-awesome-project/runs/jr0mzoz8</a><br> View project at: <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/my-awesome-project' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/my-awesome-project</a><br>Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250227_220535-jr0mzoz8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize a W&B run\n",
    "wandb.init(\n",
    "    entity=\"bullseye2608-indian-institute-of-technology-madras\",\n",
    "    project=\"my-awesome-project\", \n",
    "    name=\"Images_\"+datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    ")\n",
    "\n",
    "# Log images to W&B\n",
    "wandb.log({\n",
    "    \"fashion_mnist_samples\": [\n",
    "        wandb.Image(X_train[np.where(y_train == i)[0][0]], caption=class_names[i])\n",
    "        for i in range(10)\n",
    "    ]\n",
    "})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # For positive values, use standard formula\n",
    "    # For negative values, use a mathematically equivalent but more stable form\n",
    "    mask = x >= 0\n",
    "    result = np.zeros_like(x, dtype=float)\n",
    "    \n",
    "    # For positive inputs: 1/(1+exp(-x))\n",
    "    result[mask] = 1 / (1 + np.exp(-x[mask]))\n",
    "    \n",
    "    # For negative inputs: exp(x)/(1+exp(x))\n",
    "    # This avoids computing exp of large positive numbers\n",
    "    exp_x = np.exp(x[~mask])\n",
    "    result[~mask] = exp_x / (1 + exp_x)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def softmax(X):\n",
    "    # Subtract the max for numerical stability\n",
    "    exps = np.exp(X - np.max(X, axis=-1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred, epsilon=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates the cross-entropy loss between true labels and predicted probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy.ndarray\n",
    "        One-hot encoded true labels or class indices.\n",
    "        If class indices, shape should be (n_samples,)\n",
    "        If one-hot encoded, shape should be (n_samples, n_classes)\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted probabilities, shape (n_samples, n_classes)\n",
    "    epsilon : float, optional\n",
    "        Small constant added to log to avoid numerical instability\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        Average cross-entropy loss across all samples\n",
    "    \"\"\"\n",
    "    # Convert y_true to one-hot if it's provided as class indices\n",
    "    if len(y_true.shape) == 1 or y_true.shape[1] == 1:\n",
    "        n_samples = len(y_true)\n",
    "        n_classes = y_pred.shape[1]\n",
    "        y_true_one_hot = np.zeros((n_samples, n_classes))\n",
    "        y_true_one_hot[np.arange(n_samples), y_true.astype(int).flatten()] = 1\n",
    "        y_true = y_true_one_hot\n",
    "    \n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Calculate cross entropy loss\n",
    "    loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def one_hot(y, transpose=False):\n",
    "    one_hot_y = np.zeros((y.size, y.max() + 1))\n",
    "    one_hot_y[np.arange(y.size), y] = 1\n",
    "    if transpose:\n",
    "        one_hot_y = one_hot_y.T\n",
    "    return one_hot_y\n",
    "\n",
    "def sigmoid_derivative(X):\n",
    "    # Ensure X is in a safe range to avoid overflow\n",
    "    X = np.clip(X, 1e-7, 1 - 1e-7)\n",
    "    return X * (1 - X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_weights_and_biases(input_size=784, num_hidden_layers=2, num_neurons=32, output_size=10):\n",
    "    weights, biases = [], []\n",
    "    if type(num_neurons)==int:\n",
    "        sizes = [input_size] + [num_neurons]*num_hidden_layers + [output_size]\n",
    "    else:\n",
    "        assert len(num_neurons)==num_hidden_layers\n",
    "        sizes = [input_size] + list(num_neurons) + [output_size]\n",
    "    \n",
    "    for i in range(1,num_hidden_layers+1):\n",
    "        weights.append(np.random.uniform(-0.5, 0.5, (sizes[i], sizes[i-1])))\n",
    "        biases.append(np.zeros(sizes[i]).reshape(1,-1))\n",
    "    \n",
    "    weights.append(np.random.uniform(-0.5, 0.5, (sizes[-1], sizes[-2])))\n",
    "    biases.append(np.zeros(sizes[-1]).reshape(1,-1))\n",
    "    \n",
    "    return weights, biases\n",
    "\n",
    "def forward_propagation(X, W, B):\n",
    "    L = len(W)\n",
    "    assert L==len(B) and L>=2\n",
    "    A, H = [], []\n",
    "    \n",
    "    A_temp = np.dot(X, W[0].T) + B[0]\n",
    "    H_temp = sigmoid(A_temp)\n",
    "    \n",
    "    A.append(A_temp)\n",
    "    H.append(H_temp)\n",
    "    \n",
    "    for i in range(1,L-1):\n",
    "        A_temp = np.dot(H[i-1], W[i].T) + B[i]\n",
    "        H_temp = sigmoid(A_temp)\n",
    "        \n",
    "        A.append(A_temp)\n",
    "        H.append(H_temp)\n",
    "    \n",
    "    A_temp = np.dot(H[L-2], W[L-1].T) + B[L-1]\n",
    "    H_temp = softmax(A_temp)\n",
    "    \n",
    "    A.append(A_temp)\n",
    "    H.append(H_temp)\n",
    "    \n",
    "    return A, H\n",
    "\n",
    "def back_propagation(X, y_true, W, A, H):\n",
    "    N = X.shape[0]\n",
    "    L = len(W)\n",
    "    assert N==y_true.size and L==len(A) and L==len(H)\n",
    "    \n",
    "    one_hot_y = one_hot(y_true)\n",
    "    y_pred = H[2]\n",
    "    dW, dB = [None] * L, [None] * L\n",
    "    \n",
    "    dA2 = y_pred - one_hot_y # NxK\n",
    "    dW2 = (np.dot(dA2.T, H[1])) / N # KxH2\n",
    "    dB2 = np.sum(dA2, axis=0).reshape(1,-1) / N # 1xK\n",
    "    \n",
    "    dA1 = np.dot(dA2, W[2]) * sigmoid_derivative(A[1]) # NxH2\n",
    "    dW1 = (np.dot(dA1.T, H[0])) / N # H2xH1\n",
    "    dB1 = np.sum(dA1, axis=0).reshape(1,-1) / N # 1xH2\n",
    "    \n",
    "    dA0 = np.dot(dA1, W[1]) * sigmoid_derivative(A[0]) # NxH1\n",
    "    dW0 = (np.dot(dA0.T, X)) / N # H1xD\n",
    "    dB0 = np.sum(dA0, axis=0).reshape(1,-1) / N # 1xH1\n",
    "    \n",
    "    dW = [dW0, dW1, dW2]\n",
    "    dB = [dB0, dB1, dB2]\n",
    "    \n",
    "    return dW, dB\n",
    "    \n",
    "def back_propagation_2(X, y_true, W, A, H):\n",
    "    N = X.shape[0]\n",
    "    L = len(W)\n",
    "    assert N==y_true.size and L==len(A) and L==len(H)\n",
    "    \n",
    "    one_hot_y = one_hot(y_true)\n",
    "    y_pred = H[L-1]\n",
    "    dW, dB = [None] * L, [None] * L\n",
    "    \n",
    "    dA = y_pred - one_hot_y # NxK\n",
    "    \n",
    "    for k in range(L-1, 0, -1):\n",
    "        dWk = (np.dot(dA.T, H[k-1])) / N\n",
    "        dBk = np.sum(dA, axis=0).reshape(1,-1) / N\n",
    "        \n",
    "        dA = np.dot(dA, W[k]) * sigmoid_derivative(A[k-1])\n",
    "        \n",
    "        dW[k] = dWk\n",
    "        dB[k] = dBk\n",
    "    \n",
    "    dW0 = (np.dot(dA.T, X)) / N\n",
    "    dB0 = np.sum(dA, axis=0).reshape(1,-1) / N\n",
    "    \n",
    "    dW[0] = dW0\n",
    "    dB[0] = dB0\n",
    "    \n",
    "    \n",
    "    return dW, dB\n",
    "    \n",
    "def gradient_descent(X_training, y_true, X_val, y_val, W, B, learning_rate=0.01, num_iterations=1000, log_every=100):\n",
    "    L = len(W)\n",
    "    training_error = np.zeros(num_iterations//log_every)\n",
    "    validation_error = np.zeros(num_iterations//log_every)\n",
    "    for i in range(num_iterations):\n",
    "        A, H = forward_propagation(X_training, W, B)\n",
    "        dW, dB = back_propagation_2(X_training, y_true, W, A, H)\n",
    "        \n",
    "        for j in range(L):\n",
    "            \n",
    "            W[j] -= learning_rate * dW[j]\n",
    "            B[j] -= learning_rate * dB[j]\n",
    "        \n",
    "        if i%log_every==0:\n",
    "            print(f'Loss after {i} iterations: {cross_entropy_loss(one_hot(y_true), H[-1])}')\n",
    "            training_error[i//log_every] = cross_entropy_loss(one_hot(y_true), H[-1])\n",
    "            A_val, H_val = forward_propagation(X_val, W, B)\n",
    "            validation_error[i//log_every] = cross_entropy_loss(one_hot(y_val), H_val[-1])\n",
    "        \n",
    "            \n",
    "    \n",
    "    return W, B\n",
    "\n",
    "def gradient_descent_with_momentum(X, y_true, W, B, learning_rate=0.01, momentum=0.4, num_iterations=100, log_every=100):\n",
    "    L = len(W)\n",
    "    \n",
    "    # Initialize velocity vectors with zeros (same shape as theta)\n",
    "    velocity_W = [np.zeros_like(W[j]) for j in range(L)]\n",
    "    velocity_B = [np.zeros_like(B[j]) for j in range(L)]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Forward and backward passes\n",
    "        A, H = forward_propagation(X, W, B)\n",
    "        dW, dB = back_propagation_2(X, y_true, W, A, H)\n",
    "        \n",
    "        # Update with momentum\n",
    "        for j in range(L):\n",
    "            # Update velocity vectors\n",
    "            velocity_W[j] = momentum * velocity_W[j] - learning_rate * dW[j]\n",
    "            velocity_B[j] = momentum * velocity_B[j] - learning_rate * dB[j]\n",
    "            \n",
    "            # Update parameters using velocities\n",
    "            W[j] += velocity_W[j]\n",
    "            B[j] += velocity_B[j]\n",
    "        \n",
    "        # Print loss periodically\n",
    "        if i % log_every == 0:\n",
    "            print(f'Loss after {i} iterations: {cross_entropy_loss(one_hot(y_true), H[-1])}')\n",
    "    \n",
    "    return W, B\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def test_model(X, y_true, W, B):\n",
    "    A, H = forward_propagation(X, W, B)\n",
    "    y_pred = np.argmax(H[-1], axis=1)\n",
    "    print(f'Accuracy: {accuracy(y_true, y_pred)}')\n",
    "    return accuracy(y_true, y_pred)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 19\n",
    "\n",
    "W, B = initialise_weights_and_biases(input_size=784, \n",
    "                            num_hidden_layers=1, \n",
    "                            num_neurons=41, \n",
    "                            output_size=10)\n",
    "\n",
    "A, H = forward_propagation(X=X_train_flat[:num_trials,:], W=W, B=B)\n",
    "\n",
    "dW, dB = back_propagation_2(X=X_train_flat[:num_trials,:], y_true=y_train[:num_trials], W=W, A=A, H=H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iterations: 2.4794911390802543\n",
      "Loss after 100 iterations: 2.3100971387366136\n",
      "Loss after 200 iterations: 2.3070401218241057\n",
      "Loss after 300 iterations: 2.3043496454801415\n",
      "Loss after 400 iterations: 2.3019473853354846\n",
      "Loss after 500 iterations: 2.299782538467519\n",
      "Loss after 600 iterations: 2.2978115876749605\n",
      "Loss after 700 iterations: 2.2960054203113067\n",
      "Loss after 800 iterations: 2.2943383629156933\n",
      "Loss after 900 iterations: 2.292786905395302\n",
      "Loss after 1000 iterations: 2.2913326623615404\n",
      "Loss after 1100 iterations: 2.289961018552156\n",
      "Loss after 1200 iterations: 2.2886614949328465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m num_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[0;32m      3\u001b[0m W, B \u001b[38;5;241m=\u001b[39m initialise_weights_and_biases(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, \n\u001b[0;32m      4\u001b[0m                             num_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m      5\u001b[0m                             num_neurons\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[0;32m      6\u001b[0m                             output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m W_new, B_new \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_trials\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[157], line 102\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(X, y_true, W, B, learning_rate, num_iterations, log_every)\u001b[0m\n\u001b[0;32m     99\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(W)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# print(i)\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     A, H \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     dW, dB \u001b[38;5;241m=\u001b[39m back_propagation_2(X, y_true, W, A, H)\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# print([f'{t.shape = }' for t in dL_dW])\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# print([f'{t.shape = }' for t in dL_dB])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[157], line 30\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(X, W, B)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     29\u001b[0m     A_temp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(H[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], W[i]\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m B[i]\n\u001b[1;32m---> 30\u001b[0m     H_temp \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_temp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     A\u001b[38;5;241m.\u001b[39mappend(A_temp)\n\u001b[0;32m     33\u001b[0m     H\u001b[38;5;241m.\u001b[39mappend(H_temp)\n",
      "Cell \u001b[1;32mIn[87], line 12\u001b[0m, in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      8\u001b[0m result[mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mx[mask]))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# For negative inputs: exp(x)/(1+exp(x))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# This avoids computing exp of large positive numbers\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m exp_x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m result[\u001b[38;5;241m~\u001b[39mmask] \u001b[38;5;241m=\u001b[39m exp_x \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m exp_x)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_trials = 2000\n",
    "\n",
    "W, B = initialise_weights_and_biases(input_size=784, \n",
    "                            num_hidden_layers=2, \n",
    "                            num_neurons=32, \n",
    "                            output_size=10)\n",
    "\n",
    "W_new, B_new = gradient_descent(X=X_train_flat[:num_trials,:],\n",
    "                        y_true=y_train[:num_trials],\n",
    "                        W=W, B=B,\n",
    "                        learning_rate=0.1,\n",
    "                        num_iterations=10000,\n",
    "                        log_every=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1026"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_trials = 60000\n",
    "\n",
    "W, B = initialise_weights_and_biases(input_size=784, \n",
    "                            num_hidden_layers=1, \n",
    "                            num_neurons=32, \n",
    "                            output_size=10)\n",
    "\n",
    "test_model(X=X_test_flat, y_true=y_test, W=W, B=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iterations: 0.48514154444668295\n",
      "Loss after 100 iterations: 0.4828476850289215\n",
      "Loss after 200 iterations: 0.47857999910444526\n",
      "Loss after 300 iterations: 0.4776903544899087\n",
      "Loss after 400 iterations: 0.47467012584318047\n",
      "Loss after 500 iterations: 0.4786551332616049\n",
      "Loss after 600 iterations: 0.4675180506768316\n",
      "Loss after 700 iterations: 0.4655332038737685\n",
      "Loss after 800 iterations: 0.46140897317226875\n",
      "Loss after 900 iterations: 0.4550303110271173\n",
      "Loss after 1000 iterations: 0.4566424441010105\n",
      "Loss after 1100 iterations: 0.45294048996458736\n",
      "Loss after 1200 iterations: 0.44811272127419083\n",
      "Loss after 1300 iterations: 0.45441297349559917\n",
      "Loss after 1400 iterations: 0.4427219270550769\n",
      "Loss after 1500 iterations: 0.43603545811857947\n",
      "Loss after 1600 iterations: 0.4360636680995777\n",
      "Loss after 1700 iterations: 0.435871623727895\n",
      "Loss after 1800 iterations: 0.438704125446202\n",
      "Loss after 1900 iterations: 0.4364155868088221\n",
      "Loss after 2000 iterations: 0.4313598490391894\n",
      "Loss after 2100 iterations: 0.4289742418087328\n",
      "Loss after 2200 iterations: 0.43340281022340643\n",
      "Loss after 2300 iterations: 0.4295227715199588\n",
      "Loss after 2400 iterations: 0.4262897729761079\n",
      "Loss after 2500 iterations: 0.42272938424339535\n",
      "Loss after 2600 iterations: 0.4242385420919575\n",
      "Loss after 2700 iterations: 0.42219100005678467\n",
      "Loss after 2800 iterations: 0.4150024491257225\n",
      "Loss after 2900 iterations: 0.41636639662575037\n",
      "Loss after 3000 iterations: 0.41382588896017963\n",
      "Loss after 3100 iterations: 0.4148143942467499\n",
      "Loss after 3200 iterations: 0.41758832458239603\n",
      "Loss after 3300 iterations: 0.4163407957110132\n",
      "Loss after 3400 iterations: 0.4071477619082644\n",
      "Loss after 3500 iterations: 0.41123530042627277\n",
      "Loss after 3600 iterations: 0.4042901105498449\n",
      "Loss after 3700 iterations: 0.4059131750925332\n",
      "Loss after 3800 iterations: 0.4050785000255617\n",
      "Loss after 3900 iterations: 0.409407825803155\n"
     ]
    }
   ],
   "source": [
    "W_new, B_new = gradient_descent_with_momentum(X=X_train_flat,\n",
    "                                            y_true=y_train,\n",
    "                                            W=W, B=B,\n",
    "                                            learning_rate=0.1,\n",
    "                                            momentum=0.9,\n",
    "                                            num_iterations=4000,\n",
    "                                            log_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8226666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8226666666666667"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(X=X_val_flat, y_true=y_val, W=W_new, B=B_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
