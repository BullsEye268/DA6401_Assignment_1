{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from utils.neural_network import NeuralNetwork\n",
    "from utils.wandb_classes import WandbTrainer, WandbCallback\n",
    "from utils.helper_functions import get_optimizer, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: bullseye2608 (bullseye2608-indian-institute-of-technology-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_data('fashion_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125000\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(layer_sizes=[784, 128, 128, 128, 10], \n",
    "                   activation_functions=['relu', 'relu', 'relu', 'softmax'], \n",
    "                   weight_init='xavier',\n",
    "                   weight_decay=0.0, LOG_EACH=True)\n",
    "\n",
    "H, A = nn.forward_propagation(X_train)\n",
    "loss = nn.compute_loss(H[-1], y_train)\n",
    "print(f'{nn.compute_accuracy(X_val, y_val) :>.6f}')\n",
    "\n",
    "nn.set_optimizer({'name':'nadam', 'learning_rate':0.001, 'epsilon':1e-7, 'beta1':0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running NadamOptimizer self.learning_rate = 0.001 self.beta1 = 0.95 self.beta2 = 0.999 self.epsilon = 1e-07\n",
      "Epoch 1/5, Iteration   0/844 --> Train Loss: 2.26874, Val Loss: 2.17015\n",
      "---------------------------------------- DONE ----------------------------------------\n",
      "0.8787\n"
     ]
    }
   ],
   "source": [
    "num_trial_datapoints = 54000\n",
    "\n",
    "LOG_EACH = True\n",
    "\n",
    "nn.train(X_train[:num_trial_datapoints], \n",
    "         y_train[:num_trial_datapoints], \n",
    "         X_val, y_val, \n",
    "         batch_size=64, \n",
    "         num_epochs=5, \n",
    "         loss_type='cross_entropy', \n",
    "         log_every=5000)\n",
    "\n",
    "LOG_EACH = False\n",
    "\n",
    "print('--'*20,'DONE','--'*20)\n",
    "print(nn.compute_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WANDB SWEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create New sweep\n",
    "# import yaml\n",
    "\n",
    "# with open(\"sweep_config.yaml\", \"r\") as file:\n",
    "#         sweep_config = yaml.safe_load(file)\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, \n",
    "#                        entity=\"bullseye2608-indian-institute-of-technology-madras\",\n",
    "#                        project=\"fashion_mnist_hp_search\")\n",
    "\n",
    "# # Run the sweep\n",
    "# wandb.agent(sweep_id, wandb_sweep_helper_function, count=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Continue the sweep\n",
    "\n",
    "# sweep_id_cont = \"bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/vhbqpquu\"\n",
    "# trainer = WandbTrainer()\n",
    "\n",
    "# wandb.agent(sweep_id_cont, trainer.train, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WANDB RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.helper_functions import OptimalConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Set random seeds for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Fashion MNIST class names\n",
    "# class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "#                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "\n",
    "# # Create and log a confusion matrix visualization\n",
    "# def plot_confusion_matrix(y_true, y_pred, run_id):\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "#     cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "#     # Create a DataFrame for better visualization\n",
    "#     cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "#     cm_norm_df = pd.DataFrame(cm_normalized, index=class_names, columns=class_names)\n",
    "    \n",
    "#     # Create figure with normalized confusion matrix\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "#     sns.heatmap(cm_norm_df, annot=True, fmt='.2f', cmap=cmap, \n",
    "#                 linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "#     plt.title(f'Normalized Confusion Matrix - Run {run_id+1}', fontsize=16)\n",
    "#     plt.ylabel('True Label', fontsize=12)\n",
    "#     plt.xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "#     recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "#     f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "#     # Save and return the figure\n",
    "#     cm_filename = f\"confusion_matrix_run_{run_id+1}.png\"\n",
    "#     plt.savefig(cm_filename, dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "    \n",
    "#     return cm_filename, cm, precision, recall, f1\n",
    "\n",
    "# # Run the optimal model configuration multiple times\n",
    "# def run_multiple_experiments(num_runs=17):\n",
    "    \n",
    "#     # Define optimal hyperparameter configuration\n",
    "#     optimal_config = OptimalConfig()\n",
    "    \n",
    "#     # Lists to store results from all runs\n",
    "#     all_metrics = []\n",
    "#     all_cms = []\n",
    "#     all_y_preds = []\n",
    "#     run_ids = []\n",
    "    \n",
    "#     # Create a group ID for all runs\n",
    "#     group_id = f\"optimal-config-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    \n",
    "#     # Run the model multiple times\n",
    "#     for run_id in range(num_runs):\n",
    "#         # Set a different random seed for each run\n",
    "#         np.random.seed(42 + run_id)\n",
    "        \n",
    "#         # Initialize a new wandb run\n",
    "#         run = wandb.init(\n",
    "#             project=\"fashion_mnist_hp_search\",\n",
    "#             name=f\"optimal-run-{run_id+1}\",\n",
    "#             tags=[\"optimal-config\"],\n",
    "#             group=group_id,\n",
    "#             config=optimal_config,\n",
    "#         )\n",
    "#         wandb.config.update(optimal_config)\n",
    "        \n",
    "#         config = optimal_config\n",
    "        \n",
    "#         layer_sizes = [784] + [config.hidden_size]*config.hidden_layers + [10]\n",
    "#         activation_functions = [config.activation]*config.hidden_layers + ['softmax']\n",
    "        \n",
    "#         nn = NeuralNetwork(layer_sizes=layer_sizes, \n",
    "#                         activation_functions=activation_functions,\n",
    "#                         weight_init=config.weight_init, \n",
    "#                         weight_decay=config.weight_decay)\n",
    "        \n",
    "#         wandb_callback = WandbCallback()\n",
    "        \n",
    "#         optimizer = get_optimizer(config.optimizer, config.learning_rate)\n",
    "#         nn.set_optimizer(optimizer)\n",
    "        \n",
    "#         nn.train(\n",
    "#             X_train,\n",
    "#             y_train,\n",
    "#             X_val,\n",
    "#             y_val,\n",
    "#             batch_size=config.batch_size,\n",
    "#             num_epochs=config.epochs,\n",
    "#             loss_type=config.loss,\n",
    "#             log_every=1000,\n",
    "#             callback=wandb_callback\n",
    "#         )\n",
    "        \n",
    "#         test_accuracy = nn.compute_accuracy(X_test, y_test)\n",
    "#         wandb.log({\"test_accuracy\": test_accuracy})\n",
    "        \n",
    "#         # Log the configuration\n",
    "        \n",
    "        \n",
    "#         # Train the model\n",
    "        \n",
    "#         # Evaluate the model\n",
    "#         test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "        \n",
    "#         # Get predictions\n",
    "#         y_pred_probs = model.predict(x_test)\n",
    "#         y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "#         # Store predictions\n",
    "#         all_y_preds.append(y_pred)\n",
    "        \n",
    "#         # Create and log confusion matrix\n",
    "#         cm_file, cm, precision, recall, f1 = plot_confusion_matrix(y_test_labels, y_pred, run_id)\n",
    "#         all_cms.append(cm)\n",
    "        \n",
    "#         # Log confusion matrix image\n",
    "#         wandb.log({\"confusion_matrix\": wandb.Image(cm_file)})\n",
    "        \n",
    "#         # Log interactive confusion matrix\n",
    "#         wandb.log({\"confusion_matrix_plot\": wandb.plot.confusion_matrix(\n",
    "#             probs=None,\n",
    "#             y_true=y_test_labels,\n",
    "#             preds=y_pred,\n",
    "#             class_names=class_names\n",
    "#         )})\n",
    "        \n",
    "#         # Log metrics\n",
    "#         run_metrics = {\n",
    "#             'test_loss': test_loss,\n",
    "#             'test_accuracy': test_acc,\n",
    "#             'run_id': run_id + 1\n",
    "#         }\n",
    "        \n",
    "#         # Log per-class metrics\n",
    "#         for i, class_name in enumerate(class_names):\n",
    "#             run_metrics[f\"precision_{class_name}\"] = precision[i]\n",
    "#             run_metrics[f\"recall_{class_name}\"] = recall[i]\n",
    "#             run_metrics[f\"f1_{class_name}\"] = f1[i]\n",
    "        \n",
    "#         # Log all metrics\n",
    "#         wandb.log(run_metrics)\n",
    "        \n",
    "#         # Store metrics for aggregate analysis\n",
    "#         all_metrics.append(run_metrics)\n",
    "#         run_ids.append(run.id)\n",
    "        \n",
    "#         # Finish the run\n",
    "#         wandb.finish()\n",
    "        \n",
    "#         print(f\"Completed run {run_id+1}/{num_runs} with accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "#     # Create an aggregate analysis run\n",
    "#     run = wandb.init(\n",
    "#         project=\"fashion-mnist-optimal\",\n",
    "#         name=f\"aggregate-analysis-{num_runs}-runs\",\n",
    "#         tags=[\"aggregate\", \"analysis\", group_id],\n",
    "#         group=group_id\n",
    "#     )\n",
    "    \n",
    "#     # Calculate average confusion matrix\n",
    "#     avg_cm = np.mean(all_cms, axis=0)\n",
    "#     std_cm = np.std(all_cms, axis=0)\n",
    "    \n",
    "#     # Create and log aggregate confusion matrix\n",
    "#     plt.figure(figsize=(12, 10))\n",
    "#     avg_cm_normalized = avg_cm.astype('float') / avg_cm.sum(axis=1)[:, np.newaxis]\n",
    "#     avg_cm_df = pd.DataFrame(avg_cm_normalized, index=class_names, columns=class_names)\n",
    "    \n",
    "#     sns.heatmap(avg_cm_df, annot=True, fmt='.2f', cmap='viridis', \n",
    "#                 linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "#     # Calculate aggregate metrics\n",
    "#     accuracies = [m['test_accuracy'] for m in all_metrics]\n",
    "#     mean_acc = np.mean(accuracies)\n",
    "#     std_acc = np.std(accuracies)\n",
    "    \n",
    "#     plt.title(f'Aggregate Confusion Matrix (17 Runs)\\nMean Accuracy: {mean_acc:.4f} ± {std_acc:.4f}', fontsize=16)\n",
    "#     plt.ylabel('True Label', fontsize=12)\n",
    "#     plt.xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "#     # Save and log the aggregate confusion matrix\n",
    "#     agg_cm_file = \"aggregate_confusion_matrix.png\"\n",
    "#     plt.savefig(agg_cm_file, dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "    \n",
    "#     wandb.log({\"aggregate_confusion_matrix\": wandb.Image(agg_cm_file)})\n",
    "    \n",
    "#     # Log aggregate metrics\n",
    "#     agg_metrics = {\n",
    "#         'mean_accuracy': mean_acc,\n",
    "#         'std_accuracy': std_acc,\n",
    "#         'min_accuracy': min(accuracies),\n",
    "#         'max_accuracy': max(accuracies),\n",
    "#         'num_runs': num_runs\n",
    "#     }\n",
    "    \n",
    "#     # Create a summary table with links to all runs\n",
    "#     run_table = wandb.Table(columns=[\"Run ID\", \"Accuracy\", \"Link\"])\n",
    "#     for i, (run_id, metrics) in enumerate(zip(run_ids, all_metrics)):\n",
    "#         run_link = f\"https://wandb.ai/[your-username]/fashion-mnist-optimal/runs/{run_id}\"\n",
    "#         run_table.add_data(i+1, metrics['test_accuracy'], run_link)\n",
    "    \n",
    "#     wandb.log({\"runs_summary\": run_table})\n",
    "#     wandb.log(agg_metrics)\n",
    "    \n",
    "#     # Finish the aggregate run\n",
    "#     wandb.finish()\n",
    "    \n",
    "#     return all_metrics, all_cms, all_y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "\n",
    "# Create and log a confusion matrix visualization\n",
    "def plot_confusion_matrix(y_true, y_pred, run_id):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    cm_norm_df = pd.DataFrame(cm_normalized, index=class_names, columns=class_names)\n",
    "    \n",
    "    # Create figure with normalized confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "    sns.heatmap(cm_norm_df, annot=True, fmt='.2f', cmap=cmap, \n",
    "                linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title(f'Normalized Confusion Matrix - Run {run_id+1}', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "    recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    # Save and return the figure\n",
    "    cm_filename = f\"./confusion_matrices/confusion_matrix_run_{run_id+1}.png\"\n",
    "    plt.savefig(cm_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return cm_filename, cm, precision, recall, f1\n",
    "\n",
    "# Run the optimal model configuration multiple times\n",
    "def run_multiple_experiments():\n",
    "    \n",
    "    # Define optimal hyperparameter configuration\n",
    "    \n",
    "    # Lists to store results from all runs\n",
    "    all_metrics = []\n",
    "    all_cms = []\n",
    "    all_y_preds = []\n",
    "    run_ids = []\n",
    "    \n",
    "    # Create a group ID for all runs\n",
    "    group_id = f\"optimal-config-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    \n",
    "    # Set changes in runs\n",
    "    variation = [\n",
    "        {},\n",
    "        {'num_layers': 4},\n",
    "        {'batch_size': 32, 'num_layers': 4},\n",
    "        {'epochs':15, 'batch_size': 128, 'num_layers': 5},\n",
    "    ]\n",
    "    num_runs = len(variation)\n",
    "    \n",
    "    # Run the model multiple times\n",
    "    for run_id in range(num_runs):\n",
    "        # Set a different random seed for each run\n",
    "        np.random.seed(42 + run_id)\n",
    "        config = OptimalConfig(**variation[run_id])\n",
    "        \n",
    "        # Initialize a new wandb run\n",
    "        run = wandb.init(\n",
    "            project=\"confusion_matrix_trials\",\n",
    "            name=f\"optimal-run-{run_id+1}\",\n",
    "            tags=[\"optimal-config\"],\n",
    "            group=group_id,\n",
    "            config=config,\n",
    "        )\n",
    "        wandb.config.update(config)\n",
    "        config.print_config()\n",
    "        \n",
    "        \n",
    "        layer_sizes = [784] + [config.hidden_size]*config.num_layers + [10]\n",
    "        activation_functions = [config.activation]*config.num_layers + ['softmax']\n",
    "        \n",
    "        nn = NeuralNetwork(layer_sizes=layer_sizes, \n",
    "                        activation_functions=activation_functions,\n",
    "                        weight_init=config.weight_init, \n",
    "                        weight_decay=config.weight_decay,\n",
    "                        LOG_EACH=True)\n",
    "        \n",
    "        wandb_callback = WandbCallback()\n",
    "        \n",
    "        optimizer = get_optimizer(config.optimizer, config.learning_rate)\n",
    "        nn.set_optimizer(optimizer)\n",
    "        \n",
    "        nn.train(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            batch_size=config.batch_size,\n",
    "            num_epochs=config.epochs,\n",
    "            loss_type=config.loss,\n",
    "            log_every=1000,\n",
    "            callback=wandb_callback\n",
    "        )\n",
    "        \n",
    "        test_accuracy = nn.compute_accuracy(X_val, y_val)\n",
    "        wandb.log({\"test_accuracy\": test_accuracy})\n",
    "        \n",
    "        # Evaluate the model\n",
    "        test_loss = nn.compute_loss(nn.predict(X_val), y_val)\n",
    "        test_acc = nn.compute_accuracy(X_val, y_val)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred_probs = nn.predict(X_val)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        # Store predictions\n",
    "        all_y_preds.append(y_pred)\n",
    "        \n",
    "        # Create and log confusion matrix\n",
    "        cm_file, cm, precision, recall, f1 = plot_confusion_matrix(y_val, y_pred, run_id)\n",
    "        all_cms.append(cm)\n",
    "        \n",
    "        # Log confusion matrix image\n",
    "        wandb.log({\"confusion_matrix\": wandb.Image(cm_file)})\n",
    "        \n",
    "        # Log interactive confusion matrix\n",
    "        wandb.log({\"confusion_matrix_plot\": wandb.plot.confusion_matrix(\n",
    "            probs=y_pred_probs,\n",
    "            y_true=y_val,\n",
    "            class_names=class_names\n",
    "        )})\n",
    "        \n",
    "        # Log metrics\n",
    "        run_metrics = {\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_acc,\n",
    "            'run_id': run_id + 1\n",
    "        }\n",
    "    \n",
    "        \n",
    "        # Log all metrics\n",
    "        wandb.log(run_metrics)\n",
    "        \n",
    "        # Store metrics for aggregate analysis\n",
    "        all_metrics.append(run_metrics)\n",
    "        run_ids.append(run.id)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # TODO: Remove Boiler PLate code\n",
    "        fig, cm, precision, recall, f1 = create_plotly_confusion_matrix(\n",
    "            y_true=y_val,\n",
    "            y_pred=y_pred,\n",
    "            class_names=class_names,\n",
    "            run_id=run_id\n",
    "            )\n",
    "        \n",
    "        log_plotly_confusion_matrix_to_wandb(fig, run_id)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Finish the run\n",
    "        wandb.finish()\n",
    "        \n",
    "        print(f\"Completed run {run_id+1}/{num_runs} with accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Create an aggregate analysis run\n",
    "    run = wandb.init(\n",
    "        project=\"confusion_matrix_trials\",\n",
    "        name=f\"aggregate-analysis-{num_runs}-runs\",\n",
    "        tags=[\"aggregate\", \"analysis\", group_id],\n",
    "        group=group_id\n",
    "    )\n",
    "    \n",
    "    # Calculate average confusion matrix\n",
    "    avg_cm = np.mean(all_cms, axis=0)\n",
    "    std_cm = np.std(all_cms, axis=0)\n",
    "    \n",
    "    # Create and log aggregate confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    avg_cm_normalized = avg_cm.astype('float') / avg_cm.sum(axis=1)[:, np.newaxis]\n",
    "    avg_cm_df = pd.DataFrame(avg_cm_normalized, index=class_names, columns=class_names)\n",
    "    \n",
    "    sns.heatmap(avg_cm_df, annot=True, fmt='.2f', cmap='viridis', \n",
    "                linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    accuracies = [m['test_accuracy'] for m in all_metrics]\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    \n",
    "    plt.title(f'Aggregate Confusion Matrix (17 Runs)\\nMean Accuracy: {mean_acc:.4f} ± {std_acc:.4f}', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    # Save and log the aggregate confusion matrix\n",
    "    agg_cm_file = \"./confusion_matrices/\"+\"aggregate_confusion_matrix.png\"\n",
    "    plt.savefig(agg_cm_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    wandb.log({\"aggregate_confusion_matrix\": wandb.Image(agg_cm_file)})\n",
    "    \n",
    "    # Log aggregate metrics\n",
    "    agg_metrics = {\n",
    "        'mean_accuracy': mean_acc,\n",
    "        'std_accuracy': std_acc,\n",
    "        'min_accuracy': min(accuracies),\n",
    "        'max_accuracy': max(accuracies),\n",
    "        'num_runs': num_runs\n",
    "    }\n",
    "    \n",
    "    # Create a summary table with links to all runs\n",
    "    run_table = wandb.Table(columns=[\"Run ID\", \"Accuracy\"])\n",
    "    for i, (run_id, metrics) in enumerate(zip(run_ids, all_metrics)):\n",
    "        run_table.add_data(i+1, metrics['test_accuracy'])\n",
    "    \n",
    "    wandb.log({\"runs_summary\": run_table})\n",
    "    wandb.log(agg_metrics)\n",
    "    \n",
    "    # Finish the aggregate run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return all_metrics, all_cms, all_y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import wandb\n",
    "import io\n",
    "\n",
    "def create_plotly_confusion_matrix(y_true, y_pred, class_names, run_id=0):\n",
    "    \"\"\"\n",
    "    Create an interactive confusion matrix using Plotly.js\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Ground truth (correct) target values\n",
    "    y_pred : array-like\n",
    "        Estimated targets as returned by a classifier\n",
    "    class_names : list\n",
    "        List of class names (e.g., ['T-shirt', 'Trouser', ...])\n",
    "    run_id : int\n",
    "        Run identifier for multiple runs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : plotly.graph_objects.Figure\n",
    "        Plotly figure object of the confusion matrix\n",
    "    cm : numpy.ndarray\n",
    "        Raw confusion matrix\n",
    "    \"\"\"\n",
    "    # Create confusion matrix with NumPy\n",
    "    num_classes = len(class_names)\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    \n",
    "    # Fill the confusion matrix\n",
    "    for i in range(len(y_true)):\n",
    "        true_class = y_true[i]\n",
    "        pred_class = y_pred[i]\n",
    "        cm[true_class, pred_class] += 1\n",
    "    \n",
    "    # Calculate normalized confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create heatmap using Plotly\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=cm_normalized,\n",
    "        x=class_names,\n",
    "        y=class_names,\n",
    "        colorscale='Blues',\n",
    "        text=cm,  # Show raw counts on hover\n",
    "        texttemplate=\"%{text}\",\n",
    "        hovertemplate=\"True: %{y}<br>Predicted: %{x}<br>Count: %{text}<br>Rate: %{z:.2f}<extra></extra>\",\n",
    "    ))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "    \n",
    "    # Add title and labels\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f'Confusion Matrix - Run {run_id+1}<br><sup>Accuracy: {accuracy:.4f}</sup>',\n",
    "            'y': 0.9,\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        xaxis_title='Predicted Label',\n",
    "        yaxis_title='True Label',\n",
    "        xaxis={'side': 'bottom'},\n",
    "        width=800,\n",
    "        height=800,\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "    \n",
    "    # Add diagonal line effect\n",
    "    diagonal_effect = []\n",
    "    for i in range(len(class_names)):\n",
    "        diagonal_effect.append(\n",
    "            go.Scatter(\n",
    "                x=[i-0.5, i+0.5],\n",
    "                y=[i-0.5, i+0.5],\n",
    "                mode='lines',\n",
    "                line=dict(color='rgba(0,0,0,0.3)', width=1.5),\n",
    "                showlegend=False,\n",
    "                hoverinfo='none'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Add annotations for values\n",
    "    annotations = []\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            # The text content\n",
    "            text = f\"{cm[i, j]}<br>({cm_normalized[i, j]:.2f})\"\n",
    "            \n",
    "            # Text color should contrast with cell color\n",
    "            text_color = 'white' if cm_normalized[i, j] > 0.5 else 'black'\n",
    "            \n",
    "            # Different styling for diagonal vs non-diagonal cells\n",
    "            font_weight = 'bold' if i == j else 'normal'\n",
    "            \n",
    "            annotations.append(\n",
    "                dict(\n",
    "                    x=j,\n",
    "                    y=i,\n",
    "                    text=text,\n",
    "                    font=dict(color=text_color, size=10, family='Arial', weight=font_weight),\n",
    "                    showarrow=False,\n",
    "                    align='center',\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    for effect in diagonal_effect:\n",
    "        fig.add_trace(effect)\n",
    "    \n",
    "    fig.update_layout(annotations=annotations)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "    recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    precision = np.nan_to_num(precision)\n",
    "    recall = np.nan_to_num(recall)\n",
    "    f1 = np.nan_to_num(f1)\n",
    "    \n",
    "    return fig, cm, precision, recall, f1\n",
    "\n",
    "def log_plotly_confusion_matrix_to_wandb(fig, run_id=0):\n",
    "    \"\"\"\n",
    "    Log a Plotly confusion matrix to Weights & Biases\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fig : plotly.graph_objects.Figure\n",
    "        Plotly figure to log\n",
    "    run_id : int\n",
    "        Run identifier\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    html_path : str\n",
    "        Path to the saved HTML file\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting Logging confusion matrix for run {run_id+1}\")\n",
    "    # Save the figure as HTML and PNG\n",
    "    html_path = f\"confusion_matrix_run_{run_id+1}.html\"\n",
    "    png_path = f\"confusion_matrix_run_{run_id+1}.png\"\n",
    "    \n",
    "    # Save as interactive HTML\n",
    "    pio.write_html(fig, file=html_path, auto_open=False)\n",
    "    print('saved html file')\n",
    "    \n",
    "    # Save as static PNG for wandb image logging\n",
    "    pio.write_image(fig, file=png_path)\n",
    "    print('saved png file')\n",
    "    \n",
    "    # Log both versions to wandb\n",
    "    wandb.log({\n",
    "        \"confusion_matrix_interactive\": wandb.Html(html_path),\n",
    "        \"confusion_matrix_image\": wandb.Image(png_path)\n",
    "    })\n",
    "    print('DONE')\n",
    "    return html_path\n",
    "\n",
    "# Example usage in your main script\n",
    "def run_experiment_with_plotly_cm(run_id=0):\n",
    "    # Your model training code here\n",
    "    # ...\n",
    "    \n",
    "    # After getting predictions:\n",
    "    fig, cm, precision, recall, f1 = create_plotly_confusion_matrix(\n",
    "        y_true=y_test_labels,\n",
    "        y_pred=y_pred,\n",
    "        class_names=class_names,\n",
    "        run_id=run_id\n",
    "    )\n",
    "    \n",
    "    # Log to wandb\n",
    "    html_path = log_plotly_confusion_matrix_to_wandb(fig, run_id)\n",
    "    \n",
    "    # Log additional metrics\n",
    "    metrics = {\n",
    "        'test_accuracy': accuracy,\n",
    "        'run_id': run_id + 1\n",
    "    }\n",
    "    \n",
    "    # Log per-class metrics\n",
    "    # for i, class_name in enumerate(class_names):\n",
    "    #     metrics[f\"precision_{class_name}\"] = precision[i]\n",
    "    #     metrics[f\"recall_{class_name}\"] = recall[i]\n",
    "    #     metrics[f\"f1_{class_name}\"] = f1[i]\n",
    "    \n",
    "    # wandb.log(metrics)\n",
    "    \n",
    "    return cm, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DELL\\Desktop\\Coding\\Python\\DL\\Assignment 1\\wandb\\run-20250302_152507-9bl3vqvn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/confusion_matrix_trials/runs/9bl3vqvn' target=\"_blank\">optimal-run-1</a></strong> to <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/confusion_matrix_trials' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/confusion_matrix_trials' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/confusion_matrix_trials</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/confusion_matrix_trials/runs/9bl3vqvn' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/confusion_matrix_trials/runs/9bl3vqvn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 10\n",
      "batch_size: 64\n",
      "loss: cross_entropy\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "momentum: 0.9\n",
      "beta: 0.9\n",
      "beta1: 0.9\n",
      "beta2: 0.999\n",
      "epsilon: 1e-08\n",
      "weight_decay: 0\n",
      "weight_init: xavier\n",
      "num_layers: 3\n",
      "hidden_size: 128\n",
      "activation: relu\n",
      "Running AdamOptimizer self.learning_rate = 0.001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08\n",
      "Epoch  1/10, Iteration   0/844 --> Train Loss: 2.32746, Val Loss: 2.20835\n",
      "Epoch  2/10, Iteration 156/844 --> Train Loss: 0.32779, Val Loss: 0.45803\n",
      "Epoch  3/10, Iteration 312/844 --> Train Loss: 0.37184, Val Loss: 0.36994\n",
      "Epoch  4/10, Iteration 468/844 --> Train Loss: 0.33248, Val Loss: 0.33943\n",
      "Epoch  5/10, Iteration 624/844 --> Train Loss: 0.30243, Val Loss: 0.32645\n",
      "Epoch  6/10, Iteration 780/844 --> Train Loss: 0.36369, Val Loss: 0.30531\n",
      "Epoch  8/10, Iteration  92/844 --> Train Loss: 0.18178, Val Loss: 0.30579\n",
      "Epoch  9/10, Iteration 248/844 --> Train Loss: 0.27549, Val Loss: 0.30972\n",
      "Epoch 10/10, Iteration 404/844 --> Train Loss: 0.21174, Val Loss: 0.30614\n",
      "Starting Logging confusion matrix for run 1\n"
     ]
    }
   ],
   "source": [
    "metrics, cms, y_preds = run_multiple_experiments()\n",
    "    \n",
    "print(\"All runs completed!\")\n",
    "print(f\"Average accuracy: {np.mean([m['test_accuracy'] for m in metrics]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
