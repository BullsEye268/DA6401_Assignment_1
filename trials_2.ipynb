{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.datasets import fashion_mnist\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: bullseye2608 (bullseye2608-indian-institute-of-technology-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(X, Y, val_ratio=0.2, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    split_index = int(n_samples * (1 - val_ratio))\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    X_val = X[val_indices]\n",
    "    Y_val = Y[val_indices]\n",
    "    \n",
    "    return X_train, X_val, Y_train, Y_val\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "(X_train, X_val, y_train, y_val) = create_validation_set(X_train, y_train, val_ratio=0.1, seed=42)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Create a DataFrame for the training data\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "train_df = pd.DataFrame(X_train_flat)\n",
    "train_df['label'] = y_train\n",
    "train_df['label_name'] = [class_names[label] for label in y_train]\n",
    "\n",
    "# Create a DataFrame for the validation data\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1) / 255.0\n",
    "val_df = pd.DataFrame(X_val_flat)\n",
    "val_df['label'] = y_val\n",
    "val_df['label_name'] = [class_names[label] for label in y_val]\n",
    "\n",
    "# Create a DataFrame for the test data\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "test_df = pd.DataFrame(X_test_flat)\n",
    "test_df['label'] = y_test\n",
    "test_df['label_name'] = [class_names[label] for label in y_test]\n",
    "\n",
    "LOG_EACH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback:\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def on_epoch_end(self, loss, val_accuracy):\n",
    "        wandb.log({\n",
    "            \"epoch\": self.epoch,\n",
    "            \"train_loss\": loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "        self.epoch += 1\n",
    "    \n",
    "# Base Optimizer class\n",
    "class Optimizer:\n",
    "    def __init__(self, W, B, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize optimizer with weights and biases shapes.\n",
    "        \n",
    "        Parameters:\n",
    "        W (list): List of weight matrices\n",
    "        B (list): List of bias vectors\n",
    "        **kwargs: Optimizer-specific parameters\n",
    "        \"\"\"\n",
    "        self.L = len(W)  # Number of layers\n",
    "        self.params = kwargs\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        \"\"\"\n",
    "        Update weights and biases based on gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        W (list): Current weights\n",
    "        B (list): Current biases\n",
    "        dW (list): Weight gradients\n",
    "        dB (list): Bias gradients\n",
    "        iteration (int): Current iteration number\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (new_W, new_B) updated weights and biases\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each optimizer must implement this method\")\n",
    "\n",
    "\n",
    "class SGDOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.01, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        if LOG_EACH and iteration==0:\n",
    "            print(f'Running SGDOptimizer {self.learning_rate = }')\n",
    "        for i in range(self.L):\n",
    "            W[i] -= self.learning_rate * dW[i]\n",
    "            B[i] -= self.learning_rate * dB[i]\n",
    "        return W, B\n",
    "\n",
    "\n",
    "class MomentumOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.01, momentum=0.9, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        if LOG_EACH and iteration==0:\n",
    "            print(f'Running MomentumOptimizer {self.learning_rate = } {self.momentum = }')\n",
    "        for i in range(self.L):\n",
    "            # Update velocity\n",
    "            self.v_W[i] = self.momentum * self.v_W[i] - self.learning_rate * dW[i]\n",
    "            self.v_B[i] = self.momentum * self.v_B[i] - self.learning_rate * dB[i]\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] += self.v_W[i]\n",
    "            B[i] += self.v_B[i]\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class NesterovOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.01, momentum=0.9, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        if LOG_EACH and iteration==0:\n",
    "            print(f'Running NesterovOptimizer {self.learning_rate = } {self.momentum = }')\n",
    "        W_lookahead = [None] * self.L\n",
    "        B_lookahead = [None] * self.L\n",
    "        \n",
    "        # Calculate lookahead position\n",
    "        for i in range(self.L):\n",
    "            W_lookahead[i] = W[i] + self.momentum * self.v_W[i]\n",
    "            B_lookahead[i] = B[i] + self.momentum * self.v_B[i]\n",
    "        \n",
    "        # Update velocity\n",
    "        for i in range(self.L):\n",
    "            self.v_W[i] = self.momentum * self.v_W[i] - self.learning_rate * dW[i]\n",
    "            self.v_B[i] = self.momentum * self.v_B[i] - self.learning_rate * dB[i]\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] += self.v_W[i]\n",
    "            B[i] += self.v_B[i]\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class RMSpropOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, decay_rate=0.9, epsilon=1e-8, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize cache vectors\n",
    "        self.cache_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.cache_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        if LOG_EACH and iteration==0:\n",
    "            print(f'Running RMSpropOptimizer {self.learning_rate = } {self.decay_rate = } {self.epsilon = }')\n",
    "        for i in range(self.L):\n",
    "            # Update cache with squared gradients\n",
    "            self.cache_W[i] = self.decay_rate * self.cache_W[i] + (1 - self.decay_rate) * np.square(dW[i])\n",
    "            self.cache_B[i] = self.decay_rate * self.cache_B[i] + (1 - self.decay_rate) * np.square(dB[i])\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] -= self.learning_rate * dW[i] / (np.sqrt(self.cache_W[i]) + self.epsilon)\n",
    "            B[i] -= self.learning_rate * dB[i] / (np.sqrt(self.cache_B[i]) + self.epsilon)\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize moment vectors\n",
    "        self.m_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.m_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        if LOG_EACH and iteration==0:\n",
    "            print(f'Running AdamOptimizer {self.learning_rate = } {self.beta1 = } {self.beta2 = } {self.epsilon = }')\n",
    "        t = iteration + 1  # Timestep starts at 1\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            # Update biased first and second moment estimates\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * dW[i]\n",
    "            self.m_B[i] = self.beta1 * self.m_B[i] + (1 - self.beta1) * dB[i]\n",
    "            \n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * np.square(dW[i])\n",
    "            self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * np.square(dB[i])\n",
    "            \n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_W_corrected = self.m_W[i] / (1 - self.beta1**t)\n",
    "            m_B_corrected = self.m_B[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            v_W_corrected = self.v_W[i] / (1 - self.beta2**t)\n",
    "            v_B_corrected = self.v_B[i] / (1 - self.beta2**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] -= self.learning_rate * m_W_corrected / (np.sqrt(v_W_corrected) + self.epsilon)\n",
    "            B[i] -= self.learning_rate * m_B_corrected / (np.sqrt(v_B_corrected) + self.epsilon)\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class NadamOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize moment vectors\n",
    "        self.m_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.m_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        if LOG_EACH and iteration==0:\n",
    "            print(f'Running NadamOptimizer {self.learning_rate = } {self.beta1 = } {self.beta2 = } {self.epsilon = }')\n",
    "        t = iteration + 1  # Timestep starts at 1\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            # Update biased first and second moment estimates\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * dW[i]\n",
    "            self.m_B[i] = self.beta1 * self.m_B[i] + (1 - self.beta1) * dB[i]\n",
    "            \n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * np.square(dW[i])\n",
    "            self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * np.square(dB[i])\n",
    "            \n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_W_corrected = self.m_W[i] / (1 - self.beta1**t)\n",
    "            m_B_corrected = self.m_B[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            v_W_corrected = self.v_W[i] / (1 - self.beta2**t)\n",
    "            v_B_corrected = self.v_B[i] / (1 - self.beta2**t)\n",
    "            \n",
    "            # Apply Nesterov momentum to first moment estimate\n",
    "            m_W_nesterov = self.beta1 * m_W_corrected + (1 - self.beta1) * dW[i] / (1 - self.beta1**t)\n",
    "            m_B_nesterov = self.beta1 * m_B_corrected + (1 - self.beta1) * dB[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] -= self.learning_rate * m_W_nesterov / (np.sqrt(v_W_corrected) + self.epsilon)\n",
    "            B[i] -= self.learning_rate * m_B_nesterov / (np.sqrt(v_B_corrected) + self.epsilon)\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    \"\"\"Convert numeric labels to one-hot encoded vectors\"\"\"\n",
    "    m = labels.shape[0]\n",
    "    one_hot = np.zeros((num_classes, m))\n",
    "    for i in range(m):\n",
    "        one_hot[labels[i], i] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Example of how to use the framework\n",
    "def example_usage():\n",
    "    # Generate some synthetic data\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(2, 500)  # 500 examples with 2 features\n",
    "    y = (X[0, :] > 0).astype(int)  # Binary classification\n",
    "    y_one_hot = one_hot_encode(y, 2)  # One-hot encode\n",
    "    \n",
    "    # Create a neural network with 2-5-2 architecture\n",
    "    layer_sizes = [2, 5, 2]\n",
    "    activation_functions = ['relu', 'softmax']\n",
    "    \n",
    "    nn = NeuralNetwork(layer_sizes, activation_functions)\n",
    "    \n",
    "    # Set optimizer and train\n",
    "    nn.set_optimizer('adam', learning_rate=0.01, beta1=0.9, beta2=0.999)\n",
    "    \n",
    "    # Train the network\n",
    "    history = nn.train(X, y_one_hot, batch_size=32, num_epochs=10, log_every=50)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = nn.predict(X)\n",
    "    predicted_classes = np.argmax(predictions, axis=0)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_classes == y)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return nn, history\n",
    "\n",
    "# To add a new optimizer like Eve, simply create a new class:\n",
    "class EveOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, beta1=0.9, beta2=0.999, beta3=0.999, k=0.1, K=10, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta3 = beta3\n",
    "        self.k = k\n",
    "        self.K = K\n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "        # Initialize Adam moment vectors\n",
    "        self.m_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.m_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        \n",
    "        # Eve-specific variables\n",
    "        self.d = 1  # Initial value for d\n",
    "        self.prev_loss = None  # Previous loss\n",
    "        \n",
    "    def update(self, W, B, dW, dB, iteration, current_loss=None):\n",
    "        if current_loss is None:\n",
    "            # If loss is not provided, use a placeholder (not ideal)\n",
    "            current_loss = 1.0\n",
    "            \n",
    "        t = iteration + 1  # Timestep starts at 1\n",
    "        \n",
    "        # Update d based on loss change - Eve's special feature\n",
    "        if self.prev_loss is not None:\n",
    "            delta_loss = abs((current_loss / self.prev_loss) - 1)\n",
    "            c = min(max(delta_loss, 1/self.K), self.k)\n",
    "            \n",
    "            if current_loss >= self.prev_loss:\n",
    "                self.d *= (1 + c)\n",
    "            else:\n",
    "                self.d /= (1 + c)\n",
    "        \n",
    "        self.prev_loss = current_loss\n",
    "        \n",
    "        # Update parameters using Adam with the d factor\n",
    "        for i in range(self.L):\n",
    "            # Update biased first and second moment estimates (same as Adam)\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * dW[i]\n",
    "            self.m_B[i] = self.beta1 * self.m_B[i] + (1 - self.beta1) * dB[i]\n",
    "            \n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * np.square(dW[i])\n",
    "            self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * np.square(dB[i])\n",
    "            \n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_W_corrected = self.m_W[i] / (1 - self.beta1**t)\n",
    "            m_B_corrected = self.m_B[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            v_W_corrected = self.v_W[i] / (1 - self.beta2**t)\n",
    "            v_B_corrected = self.v_B[i] / (1 - self.beta2**t)\n",
    "            \n",
    "            # Update parameters with Eve's d factor\n",
    "            W[i] -= (self.learning_rate / self.d) * m_W_corrected / (np.sqrt(v_W_corrected) + self.epsilon)\n",
    "            B[i] -= (self.learning_rate / self.d) * m_B_corrected / (np.sqrt(v_B_corrected) + self.epsilon)\n",
    "        \n",
    "        return W, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes=[784, 17, 19, 10], \n",
    "                 activation_functions=['sigmoid', 'sigmoid', 'softmax'], \n",
    "                 weight_decay=0.0, weight_init='random'):\n",
    "        assert len(layer_sizes) == len(activation_functions) + 1, \"Number of layers (excluding input layer) and activations must match\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.activation_functions = activation_functions\n",
    "        \n",
    "        if weight_init == 'random':\n",
    "            self.W = [np.random.uniform(-0.5, 0.5, (self.layer_sizes[i], self.layer_sizes[i - 1])) for i in range(1, len(self.layer_sizes))]\n",
    "            self.B = [np.zeros(self.layer_sizes[i]).reshape(1,-1) for i in range(1, len(self.layer_sizes))]\n",
    "        elif weight_init == 'xavier':\n",
    "            # Xavier/Glorot initialization for weights\n",
    "            self.W = [np.random.randn(self.layer_sizes[i], self.layer_sizes[i - 1]) * \n",
    "                    np.sqrt(2.0 / (self.layer_sizes[i] + self.layer_sizes[i - 1])) \n",
    "                    for i in range(1, len(self.layer_sizes))]\n",
    "            self.B = [np.zeros(self.layer_sizes[i]).reshape(1,-1) for i in range(1, len(self.layer_sizes))]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported weight initialization: {weight_init}\")\n",
    "        \n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.optimizer = None\n",
    "    \n",
    "    def set_optimizer(self, optimizer_dict):\n",
    "        optimizer_map = {\n",
    "            'sgd': SGDOptimizer,\n",
    "            'momentum': MomentumOptimizer,\n",
    "            'nesterov': NesterovOptimizer,\n",
    "            'rmsprop': RMSpropOptimizer,\n",
    "            'adam': AdamOptimizer,\n",
    "            'nadam': NadamOptimizer\n",
    "        }\n",
    "\n",
    "        if optimizer_dict['name'] not in optimizer_map:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer_dict['name']}\")\n",
    "        \n",
    "        self.optimizer = optimizer_map[optimizer_dict['name']](self.W, self.B, **optimizer_dict)\n",
    "    \n",
    "    def activate(self, A, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-A))\n",
    "        elif activation == 'relu':\n",
    "            return np.maximum(0, A)\n",
    "        elif activation == 'tanh':\n",
    "            return np.tanh(A)\n",
    "        elif activation == 'softmax':\n",
    "            # For numerical stability, subtract max value\n",
    "            exps = np.exp(A - np.max(A, axis=-1, keepdims=True))\n",
    "            return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def _activate_derivative(self, Z, A, activation):\n",
    "        \"\"\"Calculate derivative of activation function\"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            return A * (1 - A)\n",
    "        elif activation == 'relu':\n",
    "            return (Z > 0).astype(float)\n",
    "        elif activation == 'tanh':\n",
    "            return 1 - A**2\n",
    "        elif activation == 'softmax':\n",
    "            # This is already handled in backpropagation for softmax+cross entropy\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        H = [X]\n",
    "        A = []\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            A.append(np.dot(H[i], self.W[i].T) + self.B[i])\n",
    "            H.append(self.activate(A[i], self.activation_functions[i]))\n",
    "        \n",
    "        return H, A\n",
    "    \n",
    "    def compute_loss(self, H_final, y, loss_type='cross_entropy'):\n",
    "        if y.ndim == 1:\n",
    "            y = self.one_hot(y)\n",
    "        m = y.shape[0]  # Number of examples\n",
    "        \n",
    "        if loss_type == 'cross_entropy':\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            epsilon = 1e-15\n",
    "            loss = -np.sum(y * np.log(H_final + epsilon)) / m\n",
    "        elif loss_type == 'mse':\n",
    "            loss = np.sum((H_final - y)**2) / (2 * m)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {loss_type}\")\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def _loss_derivative(self, A_final, y, loss_type):\n",
    "        if loss_type == 'cross_entropy':\n",
    "            epsilon = 1e-15\n",
    "            return -y / (A_final + epsilon)\n",
    "        elif loss_type == 'mse':\n",
    "            return A_final - y\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {loss_type}\")\n",
    "    \n",
    "    def one_hot(self, y):\n",
    "        one_hot_y = np.zeros((y.size, self.layer_sizes[-1]))\n",
    "        one_hot_y[np.arange(y.size), y] = 1\n",
    "        return one_hot_y\n",
    "    \n",
    "    def back_propagation(self, X, y, H, A, loss_type='cross_entropy'):\n",
    "        assert len(H) == self.L + 1 and len(A) == self.L\n",
    "        N = X.shape[0]\n",
    "        assert N==y.size and self.L==len(A) and self.L + 1==len(H)\n",
    "        \n",
    "        dW, dB = [None] * self.L, [None] * self.L\n",
    "        \n",
    "        if loss_type == 'cross_entropy' and self.activation_functions[-1] == 'softmax':\n",
    "            # Gradient simplifies when using softmax + cross entropy\n",
    "            dA = H[-1] - self.one_hot(y)\n",
    "        else:\n",
    "            dH = self._loss_derivative(H[-1], y, loss_type)\n",
    "            dA = dH * self._activate_derivative(A[-1], H[-1], self.activation_functions[-1])\n",
    "            \n",
    "        dW[-1] = np.dot(dA.T, H[-2]) / N\n",
    "        dB[-1] = np.sum(dA, axis=0, keepdims=True) / N\n",
    "        \n",
    "        for k in range(self.L-2, -1, -1):\n",
    "            \n",
    "            dA = np.dot(dA, self.W[k+1]) * self._activate_derivative(A[k], H[k+1], self.activation_functions[k])\n",
    "            \n",
    "            dWk = (np.dot(dA.T, H[k])) / N\n",
    "            dBk = np.sum(dA, axis=0, keepdims=True) / N\n",
    "            dW[k] = dWk\n",
    "            dB[k] = dBk\n",
    "        \n",
    "        if self.weight_decay > 0:\n",
    "            for i in range(len(self.W)):\n",
    "                dW[i] += self.weight_decay * self.W[i]\n",
    "               \n",
    "        return dW, dB\n",
    "    \n",
    "    def plot_history(self, history):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(history['train_loss'], label='Training Loss')\n",
    "        if 'val_loss' in history:\n",
    "            plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        H, _ = self.forward_propagation(X)\n",
    "        return H[-1]\n",
    "    \n",
    "    def compute_accuracy(self, X, y):\n",
    "        y_pred = np.argmax(self.predict(X), axis=1)\n",
    "        return np.mean(y_pred == y)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              batch_size=32, num_epochs=100, loss_type='cross_entropy', \n",
    "              log_every=100, callback=None):\n",
    "        \n",
    "        if self.optimizer is None:\n",
    "            self.set_optimizer({'name':'sgd', 'learning_rate':0.01})\n",
    "        \n",
    "        num_datapoints = X_train.shape[0]\n",
    "        num_batches = int(np.ceil(num_datapoints / batch_size))\n",
    "        \n",
    "        spacer_1 = int(np.log10(num_epochs)+1)\n",
    "        spacer_2 = int(np.log10(num_batches)+1)\n",
    "        \n",
    "        history = {\n",
    "            'train_loss' : [],\n",
    "            'val_loss' : [] if X_val is not None else None\n",
    "        }\n",
    "        \n",
    "        iteration = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            permutation = np.random.permutation(num_datapoints)\n",
    "            X_train = X_train[permutation]\n",
    "            y_train = y_train[permutation]\n",
    "            \n",
    "            for batch in range(num_batches):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = min((batch + 1) * batch_size, num_datapoints)\n",
    "                X_batch = X_train[start_idx:end_idx]\n",
    "                y_batch = y_train[start_idx:end_idx]\n",
    "                \n",
    "                H, A = self.forward_propagation(X_batch)\n",
    "                dW, dB = self.back_propagation(X_batch, y_batch, H, A, loss_type)\n",
    "                \n",
    "                self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB, iteration)\n",
    "                \n",
    "                if iteration % log_every == 0:\n",
    "                    train_loss = self.compute_loss(H[-1], y_batch, loss_type)\n",
    "                    history['train_loss'].append(train_loss)\n",
    "                    \n",
    "                    if X_val is not None and y_val is not None:\n",
    "                        val_loss = self.compute_loss(self.predict(X_val), y_val, loss_type)\n",
    "                        history['val_loss'].append(val_loss)\n",
    "                        print(f\"Epoch {epoch+1 :>{spacer_1}}/{num_epochs}, Iteration {iteration%num_batches :>{spacer_2}}/{num_batches} --> Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}\")\n",
    "                    else:\n",
    "                        print(f\"Epoch {epoch+1 :>{spacer_1}}/{num_epochs}, Iteration {iteration%num_batches :>{spacer_2}}/{num_batches} --> Train Loss: {train_loss:.5f}\")\n",
    "                \n",
    "                iteration += 1\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback.on_epoch_end(train_loss, self.compute_accuracy(X_val, y_val))\n",
    "                \n",
    "        return history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RMSpropOptimizer self.learning_rate = 0.01 self.decay_rate = 0.9 self.epsilon = 1e-05\n",
      "Epoch   1/100, Iteration  0/79 --> Train Loss: 2.38844, Val Loss: 2.29104\n",
      "Epoch  64/100, Iteration 23/79 --> Train Loss: 0.28223, Val Loss: 0.58742\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(layer_sizes=[784, 16, 16, 10], \n",
    "                   activation_functions=['sigmoid', 'sigmoid', 'softmax'], \n",
    "                   weight_init='xavier',\n",
    "                   weight_decay=0.0)\n",
    "\n",
    "# H, A = nn.forward_propagation(X_train_flat)\n",
    "# loss = nn.compute_loss(H[-1], y_train)\n",
    "# nn.compute_accuracy(X_val_flat, y_val)\n",
    "\n",
    "nn.set_optimizer({'name':'rmsprop', 'learning_rate':0.01, 'beta':0.69, 'epsilon':1e-5})\n",
    "\n",
    "num_trial_datapoints = 10000\n",
    "\n",
    "hist = nn.train(X_train_flat[:num_trial_datapoints], \n",
    "         y_train[:num_trial_datapoints], \n",
    "         X_val_flat, y_val, \n",
    "         batch_size=128, \n",
    "         num_epochs=100, \n",
    "         loss_type='cross_entropy', \n",
    "         log_every=5000)\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- DONE ----------------------------------------\n",
      "0.1817\n"
     ]
    }
   ],
   "source": [
    "num_trial_datapoints = 10000\n",
    "\n",
    "nn.train(X_train_flat[:num_trial_datapoints], \n",
    "         y_train[:num_trial_datapoints], \n",
    "         X_val_flat, y_val, \n",
    "         batch_size=64, \n",
    "         num_epochs=1000, \n",
    "         loss_type='cross_entropy', \n",
    "         log_every=5000)\n",
    "\n",
    "print('--'*20,'DONE','--'*20)\n",
    "print(nn.compute_accuracy(X_test_flat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8305"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.compute_accuracy(X_test_flat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WANDB SWEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(name, learning_rate):\n",
    "    # This should match how your NeuralNetwork class handles optimizers\n",
    "    if name == 'sgd':\n",
    "        return {'name': 'sgd','learning_rate': learning_rate}\n",
    "    elif name == 'momentum':\n",
    "        return {'name': 'momentum','learning_rate': learning_rate, 'momentum': 0.9}\n",
    "    elif name == 'nesterov':\n",
    "        return {'name': 'nesterov','learning_rate': learning_rate, 'momentum': 0.9}\n",
    "    elif name == 'rmsprop':\n",
    "        return {'name': 'rmsprop','learning_rate': learning_rate, 'beta': 0.9, 'epsilon': 1e-8}\n",
    "    elif name == 'adam':\n",
    "        return {'name': 'adam','learning_rate': learning_rate, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8}\n",
    "    elif name == 'nadam':\n",
    "        return {'name': 'nadam','learning_rate': learning_rate, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8}\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer {name} not recognized\")\n",
    "    \n",
    "def wandb_sweep_helper_function(config=None):\n",
    "    with wandb.init(\n",
    "        entity=\"bullseye2608-indian-institute-of-technology-madras\",\n",
    "        project=\"fashion_mnist_hp_search\",\n",
    "        config=config):\n",
    "        \n",
    "        config = wandb.config\n",
    "        for t in config:\n",
    "            print(t)\n",
    "        \n",
    "        \n",
    "        layer_sizes = [784] + [config.hidden_size]*config.hidden_layers + [10]\n",
    "        activation_functions = [config.activation]*config.hidden_layers + ['softmax']\n",
    "        \n",
    "        nn = NeuralNetwork(layer_sizes=layer_sizes, \n",
    "                           activation_functions=activation_functions,\n",
    "                           weight_init=config.weight_init, \n",
    "                           weight_decay=config.weight_decay)\n",
    "        \n",
    "        wandb_callback = WandbCallback()\n",
    "        \n",
    "        optimizer = get_optimizer(config.optimizer, config.learning_rate)\n",
    "        nn.set_optimizer(optimizer)\n",
    "        \n",
    "        history = nn.train(\n",
    "            X_train_flat, \n",
    "            y_train, \n",
    "            X_val_flat, \n",
    "            y_val, \n",
    "            batch_size=config.batch_size, \n",
    "            num_epochs=config.epochs, \n",
    "            loss_type='cross_entropy', \n",
    "            log_every=1000,\n",
    "            callback=wandb_callback  # Assuming your NeuralNetwork class supports callbacks\n",
    "        )\n",
    "        \n",
    "        test_accuracy = nn.compute_accuracy(X_test_flat, y_test)\n",
    "        wandb.log({\"test_accuracy\": test_accuracy})\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        return history\n",
    "        \n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Bayesian optimization strategy\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs': {\n",
    "            'values': [5, 10]\n",
    "        },\n",
    "        'hidden_layers': {\n",
    "            'values': [3, 4, 5]\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0, 0.0005, 0.5]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [1e-3, 1e-4]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'weight_init': {\n",
    "            'values': ['random', 'xavier']\n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['sigmoid', 'tanh', 'relu']\n",
    "        }\n",
    "    }\n",
    "}   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: hww232ma\n",
      "Sweep URL: https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: pulvux9u with config:\n",
      "wandb: \tactivation: relu\n",
      "wandb: \tbatch_size: 16\n",
      "wandb: \tepochs: 5\n",
      "wandb: \thidden_layers: 3\n",
      "wandb: \thidden_size: 32\n",
      "wandb: \tlearning_rate: 0.0001\n",
      "wandb: \toptimizer: adam\n",
      "wandb: \tweight_decay: 0.5\n",
      "wandb: \tweight_init: random\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'fashion_mnist_hp_search' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'bullseye2608-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DELL\\Desktop\\Coding\\Python\\DL\\wandb\\run-20250301_000032-pulvux9u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/pulvux9u' target=\"_blank\">gentle-sweep-1</a></strong> to <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/pulvux9u' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/pulvux9u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_wandb\n",
      "activation\n",
      "batch_size\n",
      "epochs\n",
      "hidden_layers\n",
      "hidden_size\n",
      "learning_rate\n",
      "optimizer\n",
      "weight_decay\n",
      "weight_init\n",
      "Running AdamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08\n",
      "Epoch 1/5, Iteration    0/3375 --> Train Loss: 13.57285, Val Loss: 11.51841\n",
      "Epoch 1/5, Iteration 1000/3375 --> Train Loss: 1.77289, Val Loss: 1.89464\n",
      "Epoch 1/5, Iteration 2000/3375 --> Train Loss: 1.82183, Val Loss: 1.67952\n",
      "Epoch 1/5, Iteration 3000/3375 --> Train Loss: 1.89722, Val Loss: 1.78247\n",
      "Epoch 2/5, Iteration  625/3375 --> Train Loss: 1.89446, Val Loss: 1.98703\n",
      "Epoch 2/5, Iteration 1625/3375 --> Train Loss: 2.19870, Val Loss: 2.19154\n",
      "Epoch 2/5, Iteration 2625/3375 --> Train Loss: 2.26395, Val Loss: 2.29386\n",
      "Epoch 3/5, Iteration  250/3375 --> Train Loss: 2.29913, Val Loss: 2.30523\n",
      "Epoch 3/5, Iteration 1250/3375 --> Train Loss: 2.30097, Val Loss: 2.30478\n",
      "Epoch 3/5, Iteration 2250/3375 --> Train Loss: 2.29633, Val Loss: 2.30428\n",
      "Epoch 3/5, Iteration 3250/3375 --> Train Loss: 2.29381, Val Loss: 2.30395\n",
      "Epoch 4/5, Iteration  875/3375 --> Train Loss: 2.30501, Val Loss: 2.30358\n",
      "Epoch 4/5, Iteration 1875/3375 --> Train Loss: 2.30130, Val Loss: 2.30346\n",
      "Epoch 4/5, Iteration 2875/3375 --> Train Loss: 2.28950, Val Loss: 2.30322\n",
      "Epoch 5/5, Iteration  500/3375 --> Train Loss: 2.31333, Val Loss: 2.30316\n",
      "Epoch 5/5, Iteration 1500/3375 --> Train Loss: 2.29668, Val Loss: 2.30300\n",
      "Epoch 5/5, Iteration 2500/3375 --> Train Loss: 2.31356, Val Loss: 2.30291\n",
      "Test Accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>▁▇███</td></tr><tr><td>val_accuracy</td><td>█▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>test_accuracy</td><td>0.1</td></tr><tr><td>train_loss</td><td>2.31356</td></tr><tr><td>val_accuracy</td><td>0.10117</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-1</strong> at: <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/pulvux9u' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/pulvux9u</a><br> View project at: <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250301_000032-pulvux9u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: 0mv1blgd with config:\n",
      "wandb: \tactivation: sigmoid\n",
      "wandb: \tbatch_size: 32\n",
      "wandb: \tepochs: 10\n",
      "wandb: \thidden_layers: 5\n",
      "wandb: \thidden_size: 128\n",
      "wandb: \tlearning_rate: 0.0001\n",
      "wandb: \toptimizer: adam\n",
      "wandb: \tweight_decay: 0.0005\n",
      "wandb: \tweight_init: random\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'fashion_mnist_hp_search' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'bullseye2608-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DELL\\Desktop\\Coding\\Python\\DL\\wandb\\run-20250301_000048-0mv1blgd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/0mv1blgd' target=\"_blank\">amber-sweep-2</a></strong> to <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/0mv1blgd' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/0mv1blgd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_wandb\n",
      "activation\n",
      "batch_size\n",
      "epochs\n",
      "hidden_layers\n",
      "hidden_size\n",
      "learning_rate\n",
      "optimizer\n",
      "weight_decay\n",
      "weight_init\n",
      "Running AdamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08\n",
      "Epoch  1/10, Iteration    0/1688 --> Train Loss: 3.47220, Val Loss: 3.43083\n",
      "Epoch  1/10, Iteration 1000/1688 --> Train Loss: 0.86852, Val Loss: 0.84604\n",
      "Epoch  2/10, Iteration  312/1688 --> Train Loss: 0.46120, Val Loss: 0.62965\n",
      "Epoch  2/10, Iteration 1312/1688 --> Train Loss: 0.51938, Val Loss: 0.55846\n",
      "Epoch  3/10, Iteration  624/1688 --> Train Loss: 0.64384, Val Loss: 0.50920\n",
      "Epoch  3/10, Iteration 1624/1688 --> Train Loss: 0.37365, Val Loss: 0.48895\n",
      "Epoch  4/10, Iteration  936/1688 --> Train Loss: 0.51389, Val Loss: 0.47010\n",
      "Epoch  5/10, Iteration  248/1688 --> Train Loss: 0.43293, Val Loss: 0.45432\n",
      "Epoch  5/10, Iteration 1248/1688 --> Train Loss: 0.40199, Val Loss: 0.44782\n",
      "Epoch  6/10, Iteration  560/1688 --> Train Loss: 0.41204, Val Loss: 0.44236\n",
      "Epoch  6/10, Iteration 1560/1688 --> Train Loss: 0.47341, Val Loss: 0.43836\n",
      "Epoch  7/10, Iteration  872/1688 --> Train Loss: 0.52206, Val Loss: 0.44105\n",
      "Epoch  8/10, Iteration  184/1688 --> Train Loss: 0.26344, Val Loss: 0.42625\n",
      "Epoch  8/10, Iteration 1184/1688 --> Train Loss: 0.42272, Val Loss: 0.41416\n",
      "Epoch  9/10, Iteration  496/1688 --> Train Loss: 0.53196, Val Loss: 0.41675\n",
      "Epoch  9/10, Iteration 1496/1688 --> Train Loss: 0.78900, Val Loss: 0.41942\n",
      "Epoch 10/10, Iteration  808/1688 --> Train Loss: 0.36626, Val Loss: 0.40599\n",
      "Test Accuracy: 0.8452\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▁▃▁▂▃▂▇▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>test_accuracy</td><td>0.8452</td></tr><tr><td>train_loss</td><td>0.36626</td></tr><tr><td>val_accuracy</td><td>0.853</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-2</strong> at: <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/0mv1blgd' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/0mv1blgd</a><br> View project at: <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250301_000048-0mv1blgd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: 2yw4p749 with config:\n",
      "wandb: \tactivation: relu\n",
      "wandb: \tbatch_size: 32\n",
      "wandb: \tepochs: 10\n",
      "wandb: \thidden_layers: 4\n",
      "wandb: \thidden_size: 64\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \toptimizer: momentum\n",
      "wandb: \tweight_decay: 0.0005\n",
      "wandb: \tweight_init: random\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'fashion_mnist_hp_search' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'bullseye2608-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DELL\\Desktop\\Coding\\Python\\DL\\wandb\\run-20250301_000152-2yw4p749</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/2yw4p749' target=\"_blank\">comfy-sweep-3</a></strong> to <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/hww232ma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/2yw4p749' target=\"_blank\">https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/runs/2yw4p749</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_wandb\n",
      "activation\n",
      "batch_size\n",
      "epochs\n",
      "hidden_layers\n",
      "hidden_size\n",
      "learning_rate\n",
      "optimizer\n",
      "weight_decay\n",
      "weight_init\n"
     ]
    }
   ],
   "source": [
    "# Create sweep\n",
    "sweep_name = \"fashion_mnist_nn_sweep\"\n",
    "sweep_id = wandb.sweep(sweep_config, \n",
    "                       entity=\"bullseye2608-indian-institute-of-technology-madras\",\n",
    "                       project=\"fashion_mnist_hp_search\")\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, wandb_sweep_helper_function, count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: uy1aaz54\n",
      "Sweep URL: https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/uy1aaz54\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_model_1290387213' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 288\u001b[0m\n\u001b[0;32m    285\u001b[0m sweep_id \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39msweep(sweep_config, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfashion_mnist_hp_search\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Run the sweep (30 runs)\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m wandb\u001b[38;5;241m.\u001b[39magent(sweep_id, \u001b[43mtrain_model_1290387213\u001b[49m, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Optionally, additional code to create a report\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# This would be done in the wandb UI, but you can programmatically create a report as well\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSweep completed. Please go to W&B UI to view results and create report.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_model_1290387213' is not defined"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import wandb\n",
    "# from tensorflow.keras.datasets import fashion_mnist\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load Fashion MNIST dataset\n",
    "# (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# # Split training data to create validation set (10% of training data)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train_full, y_train_full, test_size=0.1, random_state=42\n",
    "# )\n",
    "\n",
    "# # Normalize pixel values to be between 0 and 1\n",
    "# X_train = X_train.astype('float32') / 255.0\n",
    "# X_val = X_val.astype('float32') / 255.0\n",
    "# X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# # Reshape data for the network (flatten 28x28 images to 784 vector)\n",
    "# X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "# X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "# X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# # Define a function to map optimizer names to your implementation\n",
    "# def get_optimizer(name, learning_rate):\n",
    "#     # This should match how your NeuralNetwork class handles optimizers\n",
    "#     if name == 'sgd':\n",
    "#         return {'type': 'sgd', 'learning_rate': learning_rate}\n",
    "#     elif name == 'momentum':\n",
    "#         return {'type': 'momentum', 'learning_rate': learning_rate, 'beta': 0.9}\n",
    "#     elif name == 'nesterov':\n",
    "#         return {'type': 'nesterov', 'learning_rate': learning_rate, 'beta': 0.9}\n",
    "#     elif name == 'rmsprop':\n",
    "#         return {'type': 'rmsprop', 'learning_rate': learning_rate, 'beta': 0.9, 'epsilon': 1e-8}\n",
    "#     elif name == 'adam':\n",
    "#         return {'type': 'adam', 'learning_rate': learning_rate, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8}\n",
    "#     elif name == 'nadam':\n",
    "#         return {'type': 'nadam', 'learning_rate': learning_rate, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8}\n",
    "#     else:\n",
    "#         raise ValueError(f\"Optimizer {name} not recognized\")\n",
    "\n",
    "# # Define the wandb training function that will use your NeuralNetwork class\n",
    "# def train_with_config(config=None):\n",
    "#     with wandb.init(config=config):\n",
    "#         # Access hyperparameter values through wandb.config\n",
    "#         config = wandb.config\n",
    "        \n",
    "#         # Create hidden layer architecture\n",
    "#         # Input is 784, output is 10\n",
    "#         layer_sizes = [784]\n",
    "        \n",
    "#         # Add hidden layers based on config\n",
    "#         for _ in range(config.hidden_layers):\n",
    "#             layer_sizes.append(config.hidden_size)\n",
    "        \n",
    "#         # Add output layer (10 classes for Fashion MNIST)\n",
    "#         layer_sizes.append(10)\n",
    "        \n",
    "#         # Define activation functions\n",
    "#         # All hidden layers use the same activation function from config\n",
    "#         activation_functions = [config.activation] * config.hidden_layers\n",
    "        \n",
    "#         # Output layer always uses softmax for classification\n",
    "#         activation_functions.append('softmax')\n",
    "        \n",
    "#         # Initialize your Neural Network\n",
    "#         from neural_network import NeuralNetwork  # Import your NeuralNetwork class\n",
    "        \n",
    "#         nn = NeuralNetwork(\n",
    "#             layer_sizes=layer_sizes,\n",
    "#             activation_functions=activation_functions,\n",
    "#             weight_initialization=config.weight_init,\n",
    "#             weight_decay=config.weight_decay\n",
    "#         )\n",
    "        \n",
    "#         # Get optimizer configuration\n",
    "#         optimizer = get_optimizer(config.optimizer, config.learning_rate)\n",
    "        \n",
    "#         # Create a wandb callback to log metrics during training\n",
    "#         class WandbCallback:\n",
    "#             def __init__(self):\n",
    "#                 self.epoch = 0\n",
    "            \n",
    "#             def on_epoch_end(self, loss, val_accuracy):\n",
    "#                 wandb.log({\n",
    "#                     \"epoch\": self.epoch,\n",
    "#                     \"train_loss\": loss,\n",
    "#                     \"val_accuracy\": val_accuracy\n",
    "#                 })\n",
    "#                 self.epoch += 1\n",
    "        \n",
    "#         wandb_callback = WandbCallback()\n",
    "        \n",
    "#         # Call your training function\n",
    "#         nn.train(\n",
    "#             X_train_flat, \n",
    "#             y_train, \n",
    "#             X_val_flat, \n",
    "#             y_val, \n",
    "#             batch_size=config.batch_size, \n",
    "#             num_epochs=config.epochs, \n",
    "#             loss_type='cross_entropy',\n",
    "#             optimizer=optimizer,\n",
    "#             callback=wandb_callback  # Assuming your NeuralNetwork class supports callbacks\n",
    "#         )\n",
    "        \n",
    "#         # Compute final test accuracy\n",
    "#         test_accuracy = nn.compute_accuracy(X_test_flat, y_test)\n",
    "#         wandb.log({\"test_accuracy\": test_accuracy})\n",
    "#         print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# # Define sweep configuration\n",
    "# sweep_config = {\n",
    "#     'method': 'bayes',  # Bayesian optimization strategy\n",
    "#     'metric': {\n",
    "#         'name': 'val_accuracy',\n",
    "#         'goal': 'maximize'\n",
    "#     },\n",
    "#     'parameters': {\n",
    "#         'epochs': {\n",
    "#             'values': [5, 10]\n",
    "#         },\n",
    "#         'hidden_layers': {\n",
    "#             'values': [3, 4, 5]\n",
    "#         },\n",
    "#         'hidden_size': {\n",
    "#             'values': [32, 64, 128]\n",
    "#         },\n",
    "#         'weight_decay': {\n",
    "#             'values': [0, 0.0005, 0.5]\n",
    "#         },\n",
    "#         'learning_rate': {\n",
    "#             'values': [1e-3, 1e-4]\n",
    "#         },\n",
    "#         'optimizer': {\n",
    "#             'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
    "#         },\n",
    "#         'batch_size': {\n",
    "#             'values': [16, 32, 64]\n",
    "#         },\n",
    "#         'weight_init': {\n",
    "#             'values': ['random', 'xavier']\n",
    "#         },\n",
    "#         'activation': {\n",
    "#             'values': ['sigmoid', 'tanh', 'relu']\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Create sweep\n",
    "# sweep_name = \"fashion_mnist_nn_sweep\"\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"fashion_mnist_hp_search\", name=sweep_name)\n",
    "\n",
    "# # Run the sweep\n",
    "# wandb.agent(sweep_id, train_with_config, count=30)\n",
    "\n",
    "# # The following code assumes the NeuralNetwork class doesn't have callback support\n",
    "# # If that's the case, you can modify the train_with_config function as follows:\n",
    "\n",
    "# \"\"\"\n",
    "# # Alternative implementation if your NeuralNetwork class doesn't support callbacks\n",
    "# def train_with_config_no_callback(config=None):\n",
    "#     with wandb.init(config=config):\n",
    "#         config = wandb.config\n",
    "        \n",
    "#         layer_sizes = [784]\n",
    "#         for _ in range(config.hidden_layers):\n",
    "#             layer_sizes.append(config.hidden_size)\n",
    "#         layer_sizes.append(10)\n",
    "        \n",
    "#         activation_functions = [config.activation] * config.hidden_layers\n",
    "#         activation_functions.append('softmax')\n",
    "        \n",
    "#         from neural_network import NeuralNetwork\n",
    "#         nn = NeuralNetwork(\n",
    "#             layer_sizes=layer_sizes,\n",
    "#             activation_functions=activation_functions,\n",
    "#             weight_initialization=config.weight_init,\n",
    "#             weight_decay=config.weight_decay\n",
    "#         )\n",
    "        \n",
    "#         optimizer = get_optimizer(config.optimizer, config.learning_rate)\n",
    "        \n",
    "#         # Implement custom epoch loop to log to wandb\n",
    "#         for epoch in range(config.epochs):\n",
    "#             # Train for one epoch\n",
    "#             loss = nn.train_epoch(\n",
    "#                 X_train_flat, \n",
    "#                 y_train,\n",
    "#                 batch_size=config.batch_size,\n",
    "#                 loss_type='cross_entropy',\n",
    "#                 optimizer=optimizer\n",
    "#             )\n",
    "            \n",
    "#             # Compute validation accuracy\n",
    "#             val_accuracy = nn.compute_accuracy(X_val_flat, y_val)\n",
    "            \n",
    "#             # Log to wandb\n",
    "#             wandb.log({\n",
    "#                 \"epoch\": epoch,\n",
    "#                 \"train_loss\": loss,\n",
    "#                 \"val_accuracy\": val_accuracy\n",
    "#             })\n",
    "            \n",
    "#             print(f\"Epoch {epoch+1}/{config.epochs}, Loss: {loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "#         # Compute final test accuracy\n",
    "#         test_accuracy = nn.compute_accuracy(X_test_flat, y_test)\n",
    "#         wandb.log({\"test_accuracy\": test_accuracy})\n",
    "#         print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'a2': 2, 'a3': 3}\n"
     ]
    }
   ],
   "source": [
    "kw = {'a1': 1, 'a2': 2, 'a3': 3}\n",
    "def print_ff(a1, **kwargs):\n",
    "    print(a1)\n",
    "    print(kwargs)\n",
    "print_ff(**kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
