{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.datasets import fashion_mnist\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: bullseye2608 (bullseye2608-indian-institute-of-technology-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(X, Y, val_ratio=0.2, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    split_index = int(n_samples * (1 - val_ratio))\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    X_val = X[val_indices]\n",
    "    Y_val = Y[val_indices]\n",
    "    \n",
    "    return X_train, X_val, Y_train, Y_val\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "(X_train, X_val, y_train, y_val) = create_validation_set(X_train, y_train, val_ratio=0.1, seed=42)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Create a DataFrame for the training data\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "train_df = pd.DataFrame(X_train_flat)\n",
    "train_df['label'] = y_train\n",
    "train_df['label_name'] = [class_names[label] for label in y_train]\n",
    "\n",
    "# Create a DataFrame for the validation data\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1) / 255.0\n",
    "val_df = pd.DataFrame(X_val_flat)\n",
    "val_df['label'] = y_val\n",
    "val_df['label_name'] = [class_names[label] for label in y_val]\n",
    "\n",
    "# Create a DataFrame for the test data\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "test_df = pd.DataFrame(X_test_flat)\n",
    "test_df['label'] = y_test\n",
    "test_df['label_name'] = [class_names[label] for label in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback:\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def on_epoch_end(self, loss, val_accuracy):\n",
    "        wandb.log({\n",
    "            \"epoch\": self.epoch,\n",
    "            \"train_loss\": loss,\n",
    "            \"val_accuracy\": val_accuracy\n",
    "        })\n",
    "        self.epoch += 1\n",
    "    \n",
    "# Base Optimizer class\n",
    "class Optimizer:\n",
    "    def __init__(self, W, B, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize optimizer with weights and biases shapes.\n",
    "        \n",
    "        Parameters:\n",
    "        W (list): List of weight matrices\n",
    "        B (list): List of bias vectors\n",
    "        **kwargs: Optimizer-specific parameters\n",
    "        \"\"\"\n",
    "        self.L = len(W)  # Number of layers\n",
    "        self.params = kwargs\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        \"\"\"\n",
    "        Update weights and biases based on gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        W (list): Current weights\n",
    "        B (list): Current biases\n",
    "        dW (list): Weight gradients\n",
    "        dB (list): Bias gradients\n",
    "        iteration (int): Current iteration number\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (new_W, new_B) updated weights and biases\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each optimizer must implement this method\")\n",
    "\n",
    "\n",
    "class SGDOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.01, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        for i in range(self.L):\n",
    "            W[i] -= self.learning_rate * dW[i]\n",
    "            B[i] -= self.learning_rate * dB[i]\n",
    "        return W, B\n",
    "\n",
    "\n",
    "class MomentumOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.01, momentum=0.9, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        for i in range(self.L):\n",
    "            # Update velocity\n",
    "            self.v_W[i] = self.momentum * self.v_W[i] - self.learning_rate * dW[i]\n",
    "            self.v_B[i] = self.momentum * self.v_B[i] - self.learning_rate * dB[i]\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] += self.v_W[i]\n",
    "            B[i] += self.v_B[i]\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class NesterovOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.01, momentum=0.9, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        W_lookahead = [None] * self.L\n",
    "        B_lookahead = [None] * self.L\n",
    "        \n",
    "        # Calculate lookahead position\n",
    "        for i in range(self.L):\n",
    "            W_lookahead[i] = W[i] + self.momentum * self.v_W[i]\n",
    "            B_lookahead[i] = B[i] + self.momentum * self.v_B[i]\n",
    "        \n",
    "        # Update velocity\n",
    "        for i in range(self.L):\n",
    "            self.v_W[i] = self.momentum * self.v_W[i] - self.learning_rate * dW[i]\n",
    "            self.v_B[i] = self.momentum * self.v_B[i] - self.learning_rate * dB[i]\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] += self.v_W[i]\n",
    "            B[i] += self.v_B[i]\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class RMSpropOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, decay_rate=0.9, epsilon=1e-8, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize cache vectors\n",
    "        self.cache_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.cache_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        for i in range(self.L):\n",
    "            # Update cache with squared gradients\n",
    "            self.cache_W[i] = self.decay_rate * self.cache_W[i] + (1 - self.decay_rate) * np.square(dW[i])\n",
    "            self.cache_B[i] = self.decay_rate * self.cache_B[i] + (1 - self.decay_rate) * np.square(dB[i])\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] -= self.learning_rate * dW[i] / (np.sqrt(self.cache_W[i]) + self.epsilon)\n",
    "            B[i] -= self.learning_rate * dB[i] / (np.sqrt(self.cache_B[i]) + self.epsilon)\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize moment vectors\n",
    "        self.m_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.m_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        t = iteration + 1  # Timestep starts at 1\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            # Update biased first and second moment estimates\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * dW[i]\n",
    "            self.m_B[i] = self.beta1 * self.m_B[i] + (1 - self.beta1) * dB[i]\n",
    "            \n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * np.square(dW[i])\n",
    "            self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * np.square(dB[i])\n",
    "            \n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_W_corrected = self.m_W[i] / (1 - self.beta1**t)\n",
    "            m_B_corrected = self.m_B[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            v_W_corrected = self.v_W[i] / (1 - self.beta2**t)\n",
    "            v_B_corrected = self.v_B[i] / (1 - self.beta2**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] -= self.learning_rate * m_W_corrected / (np.sqrt(v_W_corrected) + self.epsilon)\n",
    "            B[i] -= self.learning_rate * m_B_corrected / (np.sqrt(v_B_corrected) + self.epsilon)\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class NadamOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize moment vectors\n",
    "        self.m_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.m_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        t = iteration + 1  # Timestep starts at 1\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            # Update biased first and second moment estimates\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * dW[i]\n",
    "            self.m_B[i] = self.beta1 * self.m_B[i] + (1 - self.beta1) * dB[i]\n",
    "            \n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * np.square(dW[i])\n",
    "            self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * np.square(dB[i])\n",
    "            \n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_W_corrected = self.m_W[i] / (1 - self.beta1**t)\n",
    "            m_B_corrected = self.m_B[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            v_W_corrected = self.v_W[i] / (1 - self.beta2**t)\n",
    "            v_B_corrected = self.v_B[i] / (1 - self.beta2**t)\n",
    "            \n",
    "            # Apply Nesterov momentum to first moment estimate\n",
    "            m_W_nesterov = self.beta1 * m_W_corrected + (1 - self.beta1) * dW[i] / (1 - self.beta1**t)\n",
    "            m_B_nesterov = self.beta1 * m_B_corrected + (1 - self.beta1) * dB[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] -= self.learning_rate * m_W_nesterov / (np.sqrt(v_W_corrected) + self.epsilon)\n",
    "            B[i] -= self.learning_rate * m_B_nesterov / (np.sqrt(v_B_corrected) + self.epsilon)\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    \"\"\"Convert numeric labels to one-hot encoded vectors\"\"\"\n",
    "    m = labels.shape[0]\n",
    "    one_hot = np.zeros((num_classes, m))\n",
    "    for i in range(m):\n",
    "        one_hot[labels[i], i] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Example of how to use the framework\n",
    "def example_usage():\n",
    "    # Generate some synthetic data\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(2, 500)  # 500 examples with 2 features\n",
    "    y = (X[0, :] > 0).astype(int)  # Binary classification\n",
    "    y_one_hot = one_hot_encode(y, 2)  # One-hot encode\n",
    "    \n",
    "    # Create a neural network with 2-5-2 architecture\n",
    "    layer_sizes = [2, 5, 2]\n",
    "    activation_functions = ['relu', 'softmax']\n",
    "    \n",
    "    nn = NeuralNetwork(layer_sizes, activation_functions)\n",
    "    \n",
    "    # Set optimizer and train\n",
    "    nn.set_optimizer('adam', learning_rate=0.01, beta1=0.9, beta2=0.999)\n",
    "    \n",
    "    # Train the network\n",
    "    history = nn.train(X, y_one_hot, batch_size=32, num_epochs=10, log_every=50)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = nn.predict(X)\n",
    "    predicted_classes = np.argmax(predictions, axis=0)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_classes == y)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return nn, history\n",
    "\n",
    "# To add a new optimizer like Eve, simply create a new class:\n",
    "class EveOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, beta1=0.9, beta2=0.999, beta3=0.999, k=0.1, K=10, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta3 = beta3\n",
    "        self.k = k\n",
    "        self.K = K\n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "        # Initialize Adam moment vectors\n",
    "        self.m_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.m_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        \n",
    "        # Eve-specific variables\n",
    "        self.d = 1  # Initial value for d\n",
    "        self.prev_loss = None  # Previous loss\n",
    "        \n",
    "    def update(self, W, B, dW, dB, iteration, current_loss=None):\n",
    "        if current_loss is None:\n",
    "            # If loss is not provided, use a placeholder (not ideal)\n",
    "            current_loss = 1.0\n",
    "            \n",
    "        t = iteration + 1  # Timestep starts at 1\n",
    "        \n",
    "        # Update d based on loss change - Eve's special feature\n",
    "        if self.prev_loss is not None:\n",
    "            delta_loss = abs((current_loss / self.prev_loss) - 1)\n",
    "            c = min(max(delta_loss, 1/self.K), self.k)\n",
    "            \n",
    "            if current_loss >= self.prev_loss:\n",
    "                self.d *= (1 + c)\n",
    "            else:\n",
    "                self.d /= (1 + c)\n",
    "        \n",
    "        self.prev_loss = current_loss\n",
    "        \n",
    "        # Update parameters using Adam with the d factor\n",
    "        for i in range(self.L):\n",
    "            # Update biased first and second moment estimates (same as Adam)\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * dW[i]\n",
    "            self.m_B[i] = self.beta1 * self.m_B[i] + (1 - self.beta1) * dB[i]\n",
    "            \n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * np.square(dW[i])\n",
    "            self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * np.square(dB[i])\n",
    "            \n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_W_corrected = self.m_W[i] / (1 - self.beta1**t)\n",
    "            m_B_corrected = self.m_B[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            v_W_corrected = self.v_W[i] / (1 - self.beta2**t)\n",
    "            v_B_corrected = self.v_B[i] / (1 - self.beta2**t)\n",
    "            \n",
    "            # Update parameters with Eve's d factor\n",
    "            W[i] -= (self.learning_rate / self.d) * m_W_corrected / (np.sqrt(v_W_corrected) + self.epsilon)\n",
    "            B[i] -= (self.learning_rate / self.d) * m_B_corrected / (np.sqrt(v_B_corrected) + self.epsilon)\n",
    "        \n",
    "        return W, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    # TODO: Weight Initialisation as 'random' or 'xavier'\n",
    "    # TODO: Weight Decay\n",
    "    def __init__(self, layer_sizes=[784, 17, 19, 10], activation_functions=['sigmoid', 'sigmoid', 'softmax']):\n",
    "        assert len(layer_sizes) - 1 == len(activation_functions), \"Number of layers (excluding input layer) and activations must match\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.activation_functions = activation_functions\n",
    "        \n",
    "        self.W = [np.random.uniform(-0.5, 0.5, (self.layer_sizes[i], self.layer_sizes[i - 1])) for i in range(1, len(self.layer_sizes))]\n",
    "        self.B = [np.zeros(self.layer_sizes[i]).reshape(1,-1) for i in range(1, len(self.layer_sizes))]\n",
    "        \n",
    "        self.optimizer = None\n",
    "    \n",
    "    def set_optimizer(self, optimizer_name, **kwargs):\n",
    "        optimizer_map = {\n",
    "            'sgd': SGDOptimizer,\n",
    "            'momentum': MomentumOptimizer,\n",
    "            'nesterov': NesterovOptimizer,\n",
    "            'rmsprop': RMSpropOptimizer,\n",
    "            'adam': AdamOptimizer,\n",
    "            'nadam': NadamOptimizer\n",
    "        }\n",
    "        \n",
    "        if optimizer_name not in optimizer_map:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "        \n",
    "        self.optimizer = optimizer_map[optimizer_name](self.W, self.B, **kwargs)\n",
    "    \n",
    "    def activate(self, A, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-A))\n",
    "        elif activation == 'relu':\n",
    "            return np.maximum(0, A)\n",
    "        elif activation == 'tanh':\n",
    "            return np.tanh(A)\n",
    "        elif activation == 'softmax':\n",
    "            # For numerical stability, subtract max value\n",
    "            exps = np.exp(A - np.max(A, axis=-1, keepdims=True))\n",
    "            return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def _activate_derivative(self, Z, A, activation):\n",
    "        \"\"\"Calculate derivative of activation function\"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            return A * (1 - A)\n",
    "        elif activation == 'relu':\n",
    "            return (Z > 0).astype(float)\n",
    "        elif activation == 'tanh':\n",
    "            return 1 - A**2\n",
    "        elif activation == 'softmax':\n",
    "            # This is already handled in backpropagation for softmax+cross entropy\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        H = [X]\n",
    "        A = []\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            A.append(np.dot(H[i], self.W[i].T) + self.B[i])\n",
    "            H.append(self.activate(A[i], self.activation_functions[i]))\n",
    "        \n",
    "        return H, A\n",
    "    \n",
    "    def compute_loss(self, H_final, y, loss_type='cross_entropy'):\n",
    "        if y.ndim == 1:\n",
    "            y = self.one_hot(y)\n",
    "        m = y.shape[0]  # Number of examples\n",
    "        \n",
    "        if loss_type == 'cross_entropy':\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            epsilon = 1e-15\n",
    "            loss = -np.sum(y * np.log(H_final + epsilon)) / m\n",
    "        elif loss_type == 'mse':\n",
    "            loss = np.sum((H_final - y)**2) / (2 * m)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {loss_type}\")\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def _loss_derivative(self, A_final, y, loss_type):\n",
    "        if loss_type == 'cross_entropy':\n",
    "            epsilon = 1e-15\n",
    "            return -y / (A_final + epsilon)\n",
    "        elif loss_type == 'mse':\n",
    "            return A_final - y\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {loss_type}\")\n",
    "    \n",
    "    def one_hot(self, y):\n",
    "        one_hot_y = np.zeros((y.size, self.layer_sizes[-1]))\n",
    "        one_hot_y[np.arange(y.size), y] = 1\n",
    "        return one_hot_y\n",
    "    \n",
    "    def back_propagation(self, X, y, H, A, loss_type='cross_entropy'):\n",
    "        assert len(H) == self.L + 1 and len(A) == self.L\n",
    "        N = X.shape[0]\n",
    "        assert N==y.size and self.L==len(A) and self.L + 1==len(H)\n",
    "        \n",
    "        dW, dB = [None] * self.L, [None] * self.L\n",
    "        \n",
    "        if loss_type == 'cross_entropy' and self.activation_functions[-1] == 'softmax':\n",
    "            # Gradient simplifies when using softmax + cross entropy\n",
    "            dA = H[-1] - self.one_hot(y)\n",
    "        else:\n",
    "            dH = self._loss_derivative(H[-1], y, loss_type)\n",
    "            dA = dH * self._activate_derivative(A[-1], H[-1], self.activation_functions[-1])\n",
    "            \n",
    "        dW[-1] = np.dot(dA.T, H[-2]) / N\n",
    "        dB[-1] = np.sum(dA, axis=0, keepdims=True) / N\n",
    "        \n",
    "        for k in range(self.L-2, -1, -1):\n",
    "            \n",
    "            dA = np.dot(dA, self.W[k+1]) * self._activate_derivative(A[k], H[k+1], self.activation_functions[k])\n",
    "            \n",
    "            dWk = (np.dot(dA.T, H[k])) / N\n",
    "            dBk = np.sum(dA, axis=0, keepdims=True) / N\n",
    "            dW[k] = dWk\n",
    "            dB[k] = dBk\n",
    "               \n",
    "        return dW, dB\n",
    "    \n",
    "    def predict(self, X):\n",
    "        H, _ = self.forward_propagation(X)\n",
    "        return H[-1]\n",
    "    \n",
    "    def compute_accuracy(self, X, y):\n",
    "        y_pred = np.argmax(self.predict(X), axis=1)\n",
    "        return np.mean(y_pred == y)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              batch_size=32, num_epochs=100, loss_type='cross_entropy', \n",
    "              log_every=100, config=None):\n",
    "        \n",
    "        if self.optimizer is None:\n",
    "            self.set_optimizer('sgd', learning_rate=0.01)\n",
    "        \n",
    "        num_datapoints = X_train.shape[0]\n",
    "        num_batches = int(np.ceil(num_datapoints / batch_size))\n",
    "        \n",
    "        spacer_1 = int(np.log10(num_epochs)+1)\n",
    "        spacer_2 = int(np.log10(num_batches)+1)\n",
    "        \n",
    "        history = {\n",
    "            'train_loss' : [],\n",
    "            'val_loss' : [] if X_val is not None else None\n",
    "        }\n",
    "        \n",
    "        iteration = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            permutation = np.random.permutation(num_datapoints)\n",
    "            X_train = X_train[permutation]\n",
    "            y_train = y_train[permutation]\n",
    "            \n",
    "            for batch in range(num_batches):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = min((batch + 1) * batch_size, num_datapoints)\n",
    "                X_batch = X_train[start_idx:end_idx]\n",
    "                y_batch = y_train[start_idx:end_idx]\n",
    "                \n",
    "                H, A = self.forward_propagation(X_batch)\n",
    "                dW, dB = self.back_propagation(X_batch, y_batch, H, A, loss_type)\n",
    "                \n",
    "                self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB, iteration)\n",
    "                \n",
    "                if iteration % log_every == 0:\n",
    "                    train_loss = self.compute_loss(H[-1], y_batch, loss_type)\n",
    "                    history['train_loss'].append(train_loss)\n",
    "                    \n",
    "                    if X_val is not None and y_val is not None:\n",
    "                        val_loss = self.compute_loss(self.predict(X_val), y_val, loss_type)\n",
    "                        history['val_loss'].append(val_loss)\n",
    "                        print(f\"Epoch {epoch+1 :>{spacer_1}}/{num_epochs}, Iteration {iteration%num_batches :>{spacer_2}}/{num_batches} --> Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}\")\n",
    "                    else:\n",
    "                        print(f\"Epoch {epoch+1 :>{spacer_1}}/{num_epochs}, Iteration {iteration%num_batches :>{spacer_2}}/{num_batches} --> Train Loss: {train_loss:.5f}\")\n",
    "                \n",
    "                iteration += 1\n",
    "                \n",
    "        return history\n",
    "    \n",
    "    def train_with_wandb(self, X_train, y_train, X_val=None, y_val=None, callback=wandb_callback):\n",
    "        \n",
    "    \n",
    "        if self.optimizer is None:\n",
    "            self.set_optimizer('sgd', learning_rate=0.01)\n",
    "        \n",
    "        num_datapoints = X_train.shape[0]\n",
    "        num_batches = int(np.ceil(num_datapoints / batch_size))\n",
    "        \n",
    "        spacer_1 = int(np.log10(num_epochs)+1)\n",
    "        spacer_2 = int(np.log10(num_batches)+1)\n",
    "        \n",
    "        history = {\n",
    "            'train_loss' : [],\n",
    "            'val_loss' : [] if X_val is not None else None\n",
    "        }\n",
    "        \n",
    "        iteration = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            permutation = np.random.permutation(num_datapoints)\n",
    "            X_train = X_train[permutation]\n",
    "            y_train = y_train[permutation]\n",
    "            \n",
    "            for batch in range(num_batches):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = min((batch + 1) * batch_size, num_datapoints)\n",
    "                X_batch = X_train[start_idx:end_idx]\n",
    "                y_batch = y_train[start_idx:end_idx]\n",
    "                \n",
    "                H, A = self.forward_propagation(X_batch)\n",
    "                dW, dB = self.back_propagation(X_batch, y_batch, H, A, loss_type)\n",
    "                \n",
    "                self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB, iteration)\n",
    "                \n",
    "                if iteration % log_every == 0:\n",
    "                    train_loss = self.compute_loss(H[-1], y_batch, loss_type)\n",
    "                    history['train_loss'].append(train_loss)\n",
    "                    \n",
    "                    if X_val is not None and y_val is not None:\n",
    "                        val_loss = self.compute_loss(self.predict(X_val), y_val, loss_type)\n",
    "                        history['val_loss'].append(val_loss)\n",
    "                        print(f\"Epoch {epoch+1 :>{spacer_1}}/{num_epochs}, Iteration {iteration%num_batches :>{spacer_2}}/{num_batches} --> Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}\")\n",
    "                    else:\n",
    "                        print(f\"Epoch {epoch+1 :>{spacer_1}}/{num_epochs}, Iteration {iteration%num_batches :>{spacer_2}}/{num_batches} --> Train Loss: {train_loss:.5f}\")\n",
    "                \n",
    "                iteration += 1\n",
    "                \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_sweep_helper_function(config=None):\n",
    "    with wandb.init(\n",
    "        entity=\"bullseye2608-indian-institute-of-technology-madras\",\n",
    "        project=\"my-awesome-project\",\n",
    "        config=config):\n",
    "        \n",
    "        config = wandb.config\n",
    "        \n",
    "        layer_sizes = [784] + [config.hidden_size]*config.hidden_layers + [10]\n",
    "        activation_functions = [config.activation]*config.hidden_layers + ['softmax']\n",
    "        \n",
    "        nn = NeuralNetwork(layer_sizes=layer_sizes, activation_functions=activation_functions)\n",
    "        \n",
    "        nn.train_with_wandb(X_train_flat, y_train, X_val_flat, y_val, config=config)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09766666666666667"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(layer_sizes=[784, 19, 37, 10], activation_functions=['sigmoid', 'sigmoid', 'softmax'])\n",
    "\n",
    "H, A = nn.forward_propagation(X_train_flat)\n",
    "loss = nn.compute_loss(H[-1], y_train)\n",
    "nn.compute_accuracy(X_val_flat, y_val)\n",
    "\n",
    "num_trial_datapoints = 10000\n",
    "\n",
    "nn.train(X_train_flat[:num_trial_datapoints], \n",
    "         y_train[:num_trial_datapoints], \n",
    "         X_val_flat, y_val, \n",
    "         batch_size=64, \n",
    "         num_epochs=1000, \n",
    "         loss_type='cross_entropy', \n",
    "         log_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000, Iteration   0/157 --> Train Loss: 2.65683, Val Loss: 2.65174\n",
      "Epoch   32/1000, Iteration 133/157 --> Train Loss: 1.01875, Val Loss: 1.08887\n",
      "Epoch   64/1000, Iteration 109/157 --> Train Loss: 0.96116, Val Loss: 0.83183\n",
      "Epoch   96/1000, Iteration  85/157 --> Train Loss: 1.02493, Val Loss: 0.72283\n",
      "Epoch  128/1000, Iteration  61/157 --> Train Loss: 0.64649, Val Loss: 0.65294\n",
      "Epoch  160/1000, Iteration  37/157 --> Train Loss: 0.66520, Val Loss: 0.60245\n",
      "Epoch  192/1000, Iteration  13/157 --> Train Loss: 0.46197, Val Loss: 0.56630\n",
      "Epoch  223/1000, Iteration 146/157 --> Train Loss: 0.42672, Val Loss: 0.54031\n",
      "Epoch  255/1000, Iteration 122/157 --> Train Loss: 0.45493, Val Loss: 0.52193\n",
      "Epoch  287/1000, Iteration  98/157 --> Train Loss: 0.51880, Val Loss: 0.50789\n",
      "Epoch  319/1000, Iteration  74/157 --> Train Loss: 0.44559, Val Loss: 0.49679\n",
      "Epoch  351/1000, Iteration  50/157 --> Train Loss: 0.50553, Val Loss: 0.48795\n",
      "Epoch  383/1000, Iteration  26/157 --> Train Loss: 0.27686, Val Loss: 0.48082\n",
      "Epoch  415/1000, Iteration   2/157 --> Train Loss: 0.48174, Val Loss: 0.47491\n",
      "Epoch  446/1000, Iteration 135/157 --> Train Loss: 0.40808, Val Loss: 0.46932\n",
      "Epoch  478/1000, Iteration 111/157 --> Train Loss: 0.57105, Val Loss: 0.46529\n",
      "Epoch  510/1000, Iteration  87/157 --> Train Loss: 0.30141, Val Loss: 0.46207\n",
      "Epoch  542/1000, Iteration  63/157 --> Train Loss: 0.38569, Val Loss: 0.45824\n",
      "Epoch  574/1000, Iteration  39/157 --> Train Loss: 0.29066, Val Loss: 0.45555\n",
      "Epoch  606/1000, Iteration  15/157 --> Train Loss: 0.37043, Val Loss: 0.45365\n",
      "Epoch  637/1000, Iteration 148/157 --> Train Loss: 0.29520, Val Loss: 0.45203\n",
      "Epoch  669/1000, Iteration 124/157 --> Train Loss: 0.30621, Val Loss: 0.45108\n",
      "Epoch  701/1000, Iteration 100/157 --> Train Loss: 0.15767, Val Loss: 0.44950\n",
      "Epoch  733/1000, Iteration  76/157 --> Train Loss: 0.24108, Val Loss: 0.44903\n",
      "Epoch  765/1000, Iteration  52/157 --> Train Loss: 0.37705, Val Loss: 0.44783\n",
      "Epoch  797/1000, Iteration  28/157 --> Train Loss: 0.39595, Val Loss: 0.44914\n",
      "Epoch  829/1000, Iteration   4/157 --> Train Loss: 0.36267, Val Loss: 0.44845\n",
      "Epoch  860/1000, Iteration 137/157 --> Train Loss: 0.39037, Val Loss: 0.44836\n",
      "Epoch  892/1000, Iteration 113/157 --> Train Loss: 0.25030, Val Loss: 0.44886\n",
      "Epoch  924/1000, Iteration  89/157 --> Train Loss: 0.17868, Val Loss: 0.45042\n",
      "Epoch  956/1000, Iteration  65/157 --> Train Loss: 0.32977, Val Loss: 0.45096\n",
      "Epoch  988/1000, Iteration  41/157 --> Train Loss: 0.26514, Val Loss: 0.45381\n",
      "---------------------------------------- DONE ----------------------------------------\n",
      "0.8375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('--'*20,'DONE','--'*20)\n",
    "print(nn.compute_accuracy(X_test_flat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8375"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.compute_accuracy(X_test_flat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: uy1aaz54\n",
      "Sweep URL: https://wandb.ai/bullseye2608-indian-institute-of-technology-madras/fashion_mnist_hp_search/sweeps/uy1aaz54\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_model_1290387213' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 288\u001b[0m\n\u001b[0;32m    285\u001b[0m sweep_id \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39msweep(sweep_config, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfashion_mnist_hp_search\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Run the sweep (30 runs)\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m wandb\u001b[38;5;241m.\u001b[39magent(sweep_id, \u001b[43mtrain_model_1290387213\u001b[49m, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Optionally, additional code to create a report\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# This would be done in the wandb UI, but you can programmatically create a report as well\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSweep completed. Please go to W&B UI to view results and create report.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_model_1290387213' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Split training data to create validation set (10% of training data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape data for the network (flatten 28x28 images to 784 vector)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Define a function to map optimizer names to your implementation\n",
    "def get_optimizer(name, learning_rate):\n",
    "    # This should match how your NeuralNetwork class handles optimizers\n",
    "    if name == 'sgd':\n",
    "        return {'type': 'sgd', 'learning_rate': learning_rate}\n",
    "    elif name == 'momentum':\n",
    "        return {'type': 'momentum', 'learning_rate': learning_rate, 'beta': 0.9}\n",
    "    elif name == 'nesterov':\n",
    "        return {'type': 'nesterov', 'learning_rate': learning_rate, 'beta': 0.9}\n",
    "    elif name == 'rmsprop':\n",
    "        return {'type': 'rmsprop', 'learning_rate': learning_rate, 'beta': 0.9, 'epsilon': 1e-8}\n",
    "    elif name == 'adam':\n",
    "        return {'type': 'adam', 'learning_rate': learning_rate, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8}\n",
    "    elif name == 'nadam':\n",
    "        return {'type': 'nadam', 'learning_rate': learning_rate, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8}\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer {name} not recognized\")\n",
    "\n",
    "# Define the wandb training function that will use your NeuralNetwork class\n",
    "def train_with_config(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        # Access hyperparameter values through wandb.config\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Create hidden layer architecture\n",
    "        # Input is 784, output is 10\n",
    "        layer_sizes = [784]\n",
    "        \n",
    "        # Add hidden layers based on config\n",
    "        for _ in range(config.hidden_layers):\n",
    "            layer_sizes.append(config.hidden_size)\n",
    "        \n",
    "        # Add output layer (10 classes for Fashion MNIST)\n",
    "        layer_sizes.append(10)\n",
    "        \n",
    "        # Define activation functions\n",
    "        # All hidden layers use the same activation function from config\n",
    "        activation_functions = [config.activation] * config.hidden_layers\n",
    "        \n",
    "        # Output layer always uses softmax for classification\n",
    "        activation_functions.append('softmax')\n",
    "        \n",
    "        # Initialize your Neural Network\n",
    "        from neural_network import NeuralNetwork  # Import your NeuralNetwork class\n",
    "        \n",
    "        nn = NeuralNetwork(\n",
    "            layer_sizes=layer_sizes,\n",
    "            activation_functions=activation_functions,\n",
    "            weight_initialization=config.weight_init,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Get optimizer configuration\n",
    "        optimizer = get_optimizer(config.optimizer, config.learning_rate)\n",
    "        \n",
    "        # Create a wandb callback to log metrics during training\n",
    "        class WandbCallback:\n",
    "            def __init__(self):\n",
    "                self.epoch = 0\n",
    "            \n",
    "            def on_epoch_end(self, loss, val_accuracy):\n",
    "                wandb.log({\n",
    "                    \"epoch\": self.epoch,\n",
    "                    \"train_loss\": loss,\n",
    "                    \"val_accuracy\": val_accuracy\n",
    "                })\n",
    "                self.epoch += 1\n",
    "        \n",
    "        wandb_callback = WandbCallback()\n",
    "        \n",
    "        # Call your training function\n",
    "        nn.train(\n",
    "            X_train_flat, \n",
    "            y_train, \n",
    "            X_val_flat, \n",
    "            y_val, \n",
    "            batch_size=config.batch_size, \n",
    "            num_epochs=config.epochs, \n",
    "            loss_type='cross_entropy',\n",
    "            optimizer=optimizer,\n",
    "            callback=wandb_callback  # Assuming your NeuralNetwork class supports callbacks\n",
    "        )\n",
    "        \n",
    "        # Compute final test accuracy\n",
    "        test_accuracy = nn.compute_accuracy(X_test_flat, y_test)\n",
    "        wandb.log({\"test_accuracy\": test_accuracy})\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Bayesian optimization strategy\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs': {\n",
    "            'values': [5, 10]\n",
    "        },\n",
    "        'hidden_layers': {\n",
    "            'values': [3, 4, 5]\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0, 0.0005, 0.5]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [1e-3, 1e-4]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'weight_init': {\n",
    "            'values': ['random', 'xavier']\n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['sigmoid', 'tanh', 'relu']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create sweep\n",
    "sweep_name = \"fashion_mnist_nn_sweep\"\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"fashion_mnist_hp_search\", name=sweep_name)\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, train_with_config, count=30)\n",
    "\n",
    "# The following code assumes the NeuralNetwork class doesn't have callback support\n",
    "# If that's the case, you can modify the train_with_config function as follows:\n",
    "\n",
    "\"\"\"\n",
    "# Alternative implementation if your NeuralNetwork class doesn't support callbacks\n",
    "def train_with_config_no_callback(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        layer_sizes = [784]\n",
    "        for _ in range(config.hidden_layers):\n",
    "            layer_sizes.append(config.hidden_size)\n",
    "        layer_sizes.append(10)\n",
    "        \n",
    "        activation_functions = [config.activation] * config.hidden_layers\n",
    "        activation_functions.append('softmax')\n",
    "        \n",
    "        from neural_network import NeuralNetwork\n",
    "        nn = NeuralNetwork(\n",
    "            layer_sizes=layer_sizes,\n",
    "            activation_functions=activation_functions,\n",
    "            weight_initialization=config.weight_init,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        optimizer = get_optimizer(config.optimizer, config.learning_rate)\n",
    "        \n",
    "        # Implement custom epoch loop to log to wandb\n",
    "        for epoch in range(config.epochs):\n",
    "            # Train for one epoch\n",
    "            loss = nn.train_epoch(\n",
    "                X_train_flat, \n",
    "                y_train,\n",
    "                batch_size=config.batch_size,\n",
    "                loss_type='cross_entropy',\n",
    "                optimizer=optimizer\n",
    "            )\n",
    "            \n",
    "            # Compute validation accuracy\n",
    "            val_accuracy = nn.compute_accuracy(X_val_flat, y_val)\n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": loss,\n",
    "                \"val_accuracy\": val_accuracy\n",
    "            })\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{config.epochs}, Loss: {loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Compute final test accuracy\n",
    "        test_accuracy = nn.compute_accuracy(X_test_flat, y_test)\n",
    "        wandb.log({\"test_accuracy\": test_accuracy})\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
