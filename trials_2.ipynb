{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(X, Y, val_ratio=0.2, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    split_index = int(n_samples * (1 - val_ratio))\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    X_val = X[val_indices]\n",
    "    Y_val = Y[val_indices]\n",
    "    \n",
    "    return X_train, X_val, Y_train, Y_val\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "(X_train, X_val, y_train, y_val) = create_validation_set(X_train, y_train, val_ratio=0.1, seed=42)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Create a DataFrame for the training data\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "train_df = pd.DataFrame(X_train_flat)\n",
    "train_df['label'] = y_train\n",
    "train_df['label_name'] = [class_names[label] for label in y_train]\n",
    "\n",
    "# Create a DataFrame for the validation data\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1) / 255.0\n",
    "val_df = pd.DataFrame(X_val_flat)\n",
    "val_df['label'] = y_val\n",
    "val_df['label_name'] = [class_names[label] for label in y_val]\n",
    "\n",
    "# Create a DataFrame for the test data\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "test_df = pd.DataFrame(X_test_flat)\n",
    "test_df['label'] = y_test\n",
    "test_df['label_name'] = [class_names[label] for label in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base Optimizer class\n",
    "class Optimizer:\n",
    "    def __init__(self, W, B, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize optimizer with weights and biases shapes.\n",
    "        \n",
    "        Parameters:\n",
    "        W (list): List of weight matrices\n",
    "        B (list): List of bias vectors\n",
    "        **kwargs: Optimizer-specific parameters\n",
    "        \"\"\"\n",
    "        self.L = len(W)  # Number of layers\n",
    "        self.params = kwargs\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        \"\"\"\n",
    "        Update weights and biases based on gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        W (list): Current weights\n",
    "        B (list): Current biases\n",
    "        dW (list): Weight gradients\n",
    "        dB (list): Bias gradients\n",
    "        iteration (int): Current iteration number\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (new_W, new_B) updated weights and biases\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each optimizer must implement this method\")\n",
    "\n",
    "\n",
    "class SGDOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.01, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        for i in range(self.L):\n",
    "            W[i] -= self.learning_rate * dW[i]\n",
    "            B[i] -= self.learning_rate * dB[i]\n",
    "        return W, B\n",
    "\n",
    "\n",
    "class MomentumOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.01, momentum=0.9, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        for i in range(self.L):\n",
    "            # Update velocity\n",
    "            self.v_W[i] = self.momentum * self.v_W[i] - self.learning_rate * dW[i]\n",
    "            self.v_B[i] = self.momentum * self.v_B[i] - self.learning_rate * dB[i]\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] += self.v_W[i]\n",
    "            B[i] += self.v_B[i]\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class NesterovOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.01, momentum=0.9, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        W_lookahead = [None] * self.L\n",
    "        B_lookahead = [None] * self.L\n",
    "        \n",
    "        # Calculate lookahead position\n",
    "        for i in range(self.L):\n",
    "            W_lookahead[i] = W[i] + self.momentum * self.v_W[i]\n",
    "            B_lookahead[i] = B[i] + self.momentum * self.v_B[i]\n",
    "        \n",
    "        # Update velocity\n",
    "        for i in range(self.L):\n",
    "            self.v_W[i] = self.momentum * self.v_W[i] - self.learning_rate * dW[i]\n",
    "            self.v_B[i] = self.momentum * self.v_B[i] - self.learning_rate * dB[i]\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] += self.v_W[i]\n",
    "            B[i] += self.v_B[i]\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class RMSpropOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, decay_rate=0.9, epsilon=1e-8, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize cache vectors\n",
    "        self.cache_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.cache_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        for i in range(self.L):\n",
    "            # Update cache with squared gradients\n",
    "            self.cache_W[i] = self.decay_rate * self.cache_W[i] + (1 - self.decay_rate) * np.square(dW[i])\n",
    "            self.cache_B[i] = self.decay_rate * self.cache_B[i] + (1 - self.decay_rate) * np.square(dB[i])\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] -= self.learning_rate * dW[i] / (np.sqrt(self.cache_W[i]) + self.epsilon)\n",
    "            B[i] -= self.learning_rate * dB[i] / (np.sqrt(self.cache_B[i]) + self.epsilon)\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize moment vectors\n",
    "        self.m_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.m_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        t = iteration + 1  # Timestep starts at 1\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            # Update biased first and second moment estimates\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * dW[i]\n",
    "            self.m_B[i] = self.beta1 * self.m_B[i] + (1 - self.beta1) * dB[i]\n",
    "            \n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * np.square(dW[i])\n",
    "            self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * np.square(dB[i])\n",
    "            \n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_W_corrected = self.m_W[i] / (1 - self.beta1**t)\n",
    "            m_B_corrected = self.m_B[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            v_W_corrected = self.v_W[i] / (1 - self.beta2**t)\n",
    "            v_B_corrected = self.v_B[i] / (1 - self.beta2**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] -= self.learning_rate * m_W_corrected / (np.sqrt(v_W_corrected) + self.epsilon)\n",
    "            B[i] -= self.learning_rate * m_B_corrected / (np.sqrt(v_B_corrected) + self.epsilon)\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "class NadamOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize moment vectors\n",
    "        self.m_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.m_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        \n",
    "        # Initialize velocity vectors\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "    \n",
    "    def update(self, W, B, dW, dB, iteration):\n",
    "        t = iteration + 1  # Timestep starts at 1\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            # Update biased first and second moment estimates\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * dW[i]\n",
    "            self.m_B[i] = self.beta1 * self.m_B[i] + (1 - self.beta1) * dB[i]\n",
    "            \n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * np.square(dW[i])\n",
    "            self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * np.square(dB[i])\n",
    "            \n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_W_corrected = self.m_W[i] / (1 - self.beta1**t)\n",
    "            m_B_corrected = self.m_B[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            v_W_corrected = self.v_W[i] / (1 - self.beta2**t)\n",
    "            v_B_corrected = self.v_B[i] / (1 - self.beta2**t)\n",
    "            \n",
    "            # Apply Nesterov momentum to first moment estimate\n",
    "            m_W_nesterov = self.beta1 * m_W_corrected + (1 - self.beta1) * dW[i] / (1 - self.beta1**t)\n",
    "            m_B_nesterov = self.beta1 * m_B_corrected + (1 - self.beta1) * dB[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            W[i] -= self.learning_rate * m_W_nesterov / (np.sqrt(v_W_corrected) + self.epsilon)\n",
    "            B[i] -= self.learning_rate * m_B_nesterov / (np.sqrt(v_B_corrected) + self.epsilon)\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    \"\"\"Convert numeric labels to one-hot encoded vectors\"\"\"\n",
    "    m = labels.shape[0]\n",
    "    one_hot = np.zeros((num_classes, m))\n",
    "    for i in range(m):\n",
    "        one_hot[labels[i], i] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Example of how to use the framework\n",
    "def example_usage():\n",
    "    # Generate some synthetic data\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(2, 500)  # 500 examples with 2 features\n",
    "    y = (X[0, :] > 0).astype(int)  # Binary classification\n",
    "    y_one_hot = one_hot_encode(y, 2)  # One-hot encode\n",
    "    \n",
    "    # Create a neural network with 2-5-2 architecture\n",
    "    layer_sizes = [2, 5, 2]\n",
    "    activation_functions = ['relu', 'softmax']\n",
    "    \n",
    "    nn = NeuralNetwork(layer_sizes, activation_functions)\n",
    "    \n",
    "    # Set optimizer and train\n",
    "    nn.set_optimizer('adam', learning_rate=0.01, beta1=0.9, beta2=0.999)\n",
    "    \n",
    "    # Train the network\n",
    "    history = nn.train(X, y_one_hot, batch_size=32, num_epochs=10, log_every=50)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = nn.predict(X)\n",
    "    predicted_classes = np.argmax(predictions, axis=0)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_classes == y)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return nn, history\n",
    "\n",
    "# To add a new optimizer like Eve, simply create a new class:\n",
    "class EveOptimizer(Optimizer):\n",
    "    def __init__(self, W, B, learning_rate=0.001, beta1=0.9, beta2=0.999, beta3=0.999, k=0.1, K=10, **kwargs):\n",
    "        super().__init__(W, B, **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta3 = beta3\n",
    "        self.k = k\n",
    "        self.K = K\n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "        # Initialize Adam moment vectors\n",
    "        self.m_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.m_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        self.v_W = [np.zeros_like(W[i]) for i in range(self.L)]\n",
    "        self.v_B = [np.zeros_like(B[i]) for i in range(self.L)]\n",
    "        \n",
    "        # Eve-specific variables\n",
    "        self.d = 1  # Initial value for d\n",
    "        self.prev_loss = None  # Previous loss\n",
    "        \n",
    "    def update(self, W, B, dW, dB, iteration, current_loss=None):\n",
    "        if current_loss is None:\n",
    "            # If loss is not provided, use a placeholder (not ideal)\n",
    "            current_loss = 1.0\n",
    "            \n",
    "        t = iteration + 1  # Timestep starts at 1\n",
    "        \n",
    "        # Update d based on loss change - Eve's special feature\n",
    "        if self.prev_loss is not None:\n",
    "            delta_loss = abs((current_loss / self.prev_loss) - 1)\n",
    "            c = min(max(delta_loss, 1/self.K), self.k)\n",
    "            \n",
    "            if current_loss >= self.prev_loss:\n",
    "                self.d *= (1 + c)\n",
    "            else:\n",
    "                self.d /= (1 + c)\n",
    "        \n",
    "        self.prev_loss = current_loss\n",
    "        \n",
    "        # Update parameters using Adam with the d factor\n",
    "        for i in range(self.L):\n",
    "            # Update biased first and second moment estimates (same as Adam)\n",
    "            self.m_W[i] = self.beta1 * self.m_W[i] + (1 - self.beta1) * dW[i]\n",
    "            self.m_B[i] = self.beta1 * self.m_B[i] + (1 - self.beta1) * dB[i]\n",
    "            \n",
    "            self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * np.square(dW[i])\n",
    "            self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * np.square(dB[i])\n",
    "            \n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_W_corrected = self.m_W[i] / (1 - self.beta1**t)\n",
    "            m_B_corrected = self.m_B[i] / (1 - self.beta1**t)\n",
    "            \n",
    "            v_W_corrected = self.v_W[i] / (1 - self.beta2**t)\n",
    "            v_B_corrected = self.v_B[i] / (1 - self.beta2**t)\n",
    "            \n",
    "            # Update parameters with Eve's d factor\n",
    "            W[i] -= (self.learning_rate / self.d) * m_W_corrected / (np.sqrt(v_W_corrected) + self.epsilon)\n",
    "            B[i] -= (self.learning_rate / self.d) * m_B_corrected / (np.sqrt(v_B_corrected) + self.epsilon)\n",
    "        \n",
    "        return W, B\n",
    "\n",
    "# If needed, update the NeuralNetwork class to accept loss in the optimizer update\n",
    "# by modifying the train method to pass current_loss to the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes=[784, 17, 19, 10], activation_functions=['sigmoid', 'sigmoid', 'softmax']):\n",
    "        assert len(layer_sizes) - 1 == len(activation_functions), \"Number of layers (excluding input layer) and activations must match\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.activation_functions = activation_functions\n",
    "        \n",
    "        self.W = [np.random.uniform(-0.5, 0.5, (self.layer_sizes[i], self.layer_sizes[i - 1])) for i in range(1, len(self.layer_sizes))]\n",
    "        self.B = [np.zeros(self.layer_sizes[i]).reshape(1,-1) for i in range(1, len(self.layer_sizes))]\n",
    "        \n",
    "        self.optimizer = None\n",
    "    \n",
    "    def set_optimizer(self, optimizer_name, **kwargs):\n",
    "        optimizer_map = {\n",
    "            'sgd': SGDOptimizer,\n",
    "            'momentum': MomentumOptimizer,\n",
    "            'nesterov': NesterovOptimizer,\n",
    "            'rmsprop': RMSpropOptimizer,\n",
    "            'adam': AdamOptimizer,\n",
    "            'nadam': NadamOptimizer\n",
    "        }\n",
    "        \n",
    "        if optimizer_name not in optimizer_map:\n",
    "            raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "        \n",
    "        self.optimizer = optimizer_map[optimizer_name](self.W, self.B, **kwargs)\n",
    "    \n",
    "    def activate(self, A, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-A))\n",
    "        elif activation == 'relu':\n",
    "            return np.maximum(0, A)\n",
    "        elif activation == 'tanh':\n",
    "            return np.tanh(A)\n",
    "        elif activation == 'softmax':\n",
    "            # For numerical stability, subtract max value\n",
    "            exps = np.exp(A - np.max(A, axis=-1, keepdims=True))\n",
    "            return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def _activate_derivative(self, Z, A, activation):\n",
    "        \"\"\"Calculate derivative of activation function\"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            return A * (1 - A)\n",
    "        elif activation == 'relu':\n",
    "            return (Z > 0).astype(float)\n",
    "        elif activation == 'tanh':\n",
    "            return 1 - A**2\n",
    "        elif activation == 'softmax':\n",
    "            # This is already handled in backpropagation for softmax+cross entropy\n",
    "            return 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        H = [X]\n",
    "        A = []\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            A.append(np.dot(H[i], self.W[i].T) + self.B[i])\n",
    "            H.append(self.activate(A[i], self.activation_functions[i]))\n",
    "        \n",
    "        return H, A\n",
    "    \n",
    "    def compute_loss(self, H_final, y, loss_type='cross_entropy'):\n",
    "        if y.ndim == 1:\n",
    "            y = self.one_hot(y)\n",
    "        m = y.shape[0]  # Number of examples\n",
    "        \n",
    "        if loss_type == 'cross_entropy':\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            epsilon = 1e-15\n",
    "            loss = -np.sum(y * np.log(H_final + epsilon)) / m\n",
    "        elif loss_type == 'mse':\n",
    "            loss = np.sum((H_final - y)**2) / (2 * m)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {loss_type}\")\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def _loss_derivative(self, A_final, y, loss_type):\n",
    "        if loss_type == 'cross_entropy':\n",
    "            epsilon = 1e-15\n",
    "            return -y / (A_final + epsilon)\n",
    "        elif loss_type == 'mse':\n",
    "            return A_final - y\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {loss_type}\")\n",
    "    \n",
    "    def one_hot(self, y):\n",
    "        one_hot_y = np.zeros((y.size, self.layer_sizes[-1]))\n",
    "        one_hot_y[np.arange(y.size), y] = 1\n",
    "        return one_hot_y\n",
    "    \n",
    "    def back_propagation(self, X, y, H, A, loss_type='cross_entropy'):\n",
    "        assert len(H) == self.L + 1 and len(A) == self.L\n",
    "        N = X.shape[0]\n",
    "        assert N==y.size and self.L==len(A) and self.L + 1==len(H)\n",
    "        \n",
    "        dW, dB = [None] * self.L, [None] * self.L\n",
    "        \n",
    "        if loss_type == 'cross_entropy' and self.activation_functions[-1] == 'softmax':\n",
    "            # Gradient simplifies when using softmax + cross entropy\n",
    "            dA = H[-1] - self.one_hot(y)\n",
    "        else:\n",
    "            dH = self._loss_derivative(H[-1], y, loss_type)\n",
    "            dA = dH * self._activate_derivative(A[-1], H[-1], self.activation_functions[-1])\n",
    "            \n",
    "        dW[-1] = np.dot(dA.T, H[-2]) / N\n",
    "        dB[-1] = np.sum(dA, axis=0, keepdims=True) / N\n",
    "        \n",
    "        for k in range(self.L-2, -1, -1):\n",
    "            \n",
    "            dA = np.dot(dA, self.W[k+1]) * self._activate_derivative(A[k], H[k+1], self.activation_functions[k])\n",
    "            \n",
    "            dWk = (np.dot(dA.T, H[k])) / N\n",
    "            dBk = np.sum(dA, axis=0, keepdims=True) / N\n",
    "            dW[k] = dWk\n",
    "            dB[k] = dBk\n",
    "               \n",
    "        return dW, dB\n",
    "    \n",
    "    def predict(self, X):\n",
    "        H, _ = self.forward_propagation(X)\n",
    "        return H[-1]\n",
    "    \n",
    "    def compute_accuracy(self, X, y):\n",
    "        y_pred = np.argmax(self.predict(X), axis=1)\n",
    "        return np.mean(y_pred == y)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              batch_size=32, num_epochs=100, loss_type='cross_entropy', \n",
    "              log_every=100):\n",
    "        \n",
    "        if self.optimizer is None:\n",
    "            self.set_optimizer('sgd', learning_rate=0.01)\n",
    "        \n",
    "        num_datapoints = X_train.shape[0]\n",
    "        num_batches = int(np.ceil(num_datapoints / batch_size))\n",
    "        \n",
    "        spacer_1 = int(np.log10(num_epochs)+1)\n",
    "        spacer_2 = int(np.log10(num_batches)+1)\n",
    "        \n",
    "        history = {\n",
    "            'train_loss' : [],\n",
    "            'val_loss' : [] if X_val is not None else None\n",
    "        }\n",
    "        \n",
    "        iteration = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            permutation = np.random.permutation(num_datapoints)\n",
    "            X_train = X_train[permutation]\n",
    "            y_train = y_train[permutation]\n",
    "            \n",
    "            for batch in range(num_batches):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = min((batch + 1) * batch_size, num_datapoints)\n",
    "                X_batch = X_train[start_idx:end_idx]\n",
    "                y_batch = y_train[start_idx:end_idx]\n",
    "                \n",
    "                H, A = self.forward_propagation(X_batch)\n",
    "                dW, dB = self.back_propagation(X_batch, y_batch, H, A, loss_type)\n",
    "                \n",
    "                self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB, iteration)\n",
    "                \n",
    "                if iteration % log_every == 0:\n",
    "                    train_loss = self.compute_loss(H[-1], y_batch, loss_type)\n",
    "                    history['train_loss'].append(train_loss)\n",
    "                    \n",
    "                    if X_val is not None and y_val is not None:\n",
    "                        val_loss = self.compute_loss(self.predict(X_val), y_val, loss_type)\n",
    "                        history['val_loss'].append(val_loss)\n",
    "                        print(f\"Epoch {epoch+1 :>{spacer_1}}/{num_epochs}, Iteration {iteration%num_batches :>{spacer_2}}/{num_batches} --> Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}\")\n",
    "                    else:\n",
    "                        print(f\"Epoch {epoch+1 :>{spacer_1}}/{num_epochs}, Iteration {iteration%num_batches :>{spacer_2}}/{num_batches} --> Train Loss: {train_loss:.5f}\")\n",
    "                \n",
    "                iteration += 1\n",
    "                \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(layer_sizes=[784, 19, 37, 10], activation_functions=['sigmoid', 'sigmoid', 'softmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09766666666666667"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H, A = nn.forward_propagation(X_train_flat)\n",
    "loss = nn.compute_loss(H[-1], y_train)\n",
    "nn.compute_accuracy(X_val_flat, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000, Iteration   0/157 --> Train Loss: 2.65683, Val Loss: 2.65174\n",
      "Epoch   32/1000, Iteration 133/157 --> Train Loss: 1.01875, Val Loss: 1.08887\n",
      "Epoch   64/1000, Iteration 109/157 --> Train Loss: 0.96116, Val Loss: 0.83183\n",
      "Epoch   96/1000, Iteration  85/157 --> Train Loss: 1.02493, Val Loss: 0.72283\n",
      "Epoch  128/1000, Iteration  61/157 --> Train Loss: 0.64649, Val Loss: 0.65294\n",
      "Epoch  160/1000, Iteration  37/157 --> Train Loss: 0.66520, Val Loss: 0.60245\n",
      "Epoch  192/1000, Iteration  13/157 --> Train Loss: 0.46197, Val Loss: 0.56630\n",
      "Epoch  223/1000, Iteration 146/157 --> Train Loss: 0.42672, Val Loss: 0.54031\n",
      "Epoch  255/1000, Iteration 122/157 --> Train Loss: 0.45493, Val Loss: 0.52193\n",
      "Epoch  287/1000, Iteration  98/157 --> Train Loss: 0.51880, Val Loss: 0.50789\n",
      "Epoch  319/1000, Iteration  74/157 --> Train Loss: 0.44559, Val Loss: 0.49679\n",
      "Epoch  351/1000, Iteration  50/157 --> Train Loss: 0.50553, Val Loss: 0.48795\n",
      "Epoch  383/1000, Iteration  26/157 --> Train Loss: 0.27686, Val Loss: 0.48082\n",
      "Epoch  415/1000, Iteration   2/157 --> Train Loss: 0.48174, Val Loss: 0.47491\n",
      "Epoch  446/1000, Iteration 135/157 --> Train Loss: 0.40808, Val Loss: 0.46932\n",
      "Epoch  478/1000, Iteration 111/157 --> Train Loss: 0.57105, Val Loss: 0.46529\n",
      "Epoch  510/1000, Iteration  87/157 --> Train Loss: 0.30141, Val Loss: 0.46207\n",
      "Epoch  542/1000, Iteration  63/157 --> Train Loss: 0.38569, Val Loss: 0.45824\n",
      "Epoch  574/1000, Iteration  39/157 --> Train Loss: 0.29066, Val Loss: 0.45555\n",
      "Epoch  606/1000, Iteration  15/157 --> Train Loss: 0.37043, Val Loss: 0.45365\n",
      "Epoch  637/1000, Iteration 148/157 --> Train Loss: 0.29520, Val Loss: 0.45203\n",
      "Epoch  669/1000, Iteration 124/157 --> Train Loss: 0.30621, Val Loss: 0.45108\n",
      "Epoch  701/1000, Iteration 100/157 --> Train Loss: 0.15767, Val Loss: 0.44950\n",
      "Epoch  733/1000, Iteration  76/157 --> Train Loss: 0.24108, Val Loss: 0.44903\n",
      "Epoch  765/1000, Iteration  52/157 --> Train Loss: 0.37705, Val Loss: 0.44783\n",
      "Epoch  797/1000, Iteration  28/157 --> Train Loss: 0.39595, Val Loss: 0.44914\n",
      "Epoch  829/1000, Iteration   4/157 --> Train Loss: 0.36267, Val Loss: 0.44845\n",
      "Epoch  860/1000, Iteration 137/157 --> Train Loss: 0.39037, Val Loss: 0.44836\n",
      "Epoch  892/1000, Iteration 113/157 --> Train Loss: 0.25030, Val Loss: 0.44886\n",
      "Epoch  924/1000, Iteration  89/157 --> Train Loss: 0.17868, Val Loss: 0.45042\n",
      "Epoch  956/1000, Iteration  65/157 --> Train Loss: 0.32977, Val Loss: 0.45096\n",
      "Epoch  988/1000, Iteration  41/157 --> Train Loss: 0.26514, Val Loss: 0.45381\n",
      "---------------------------------------- DONE ----------------------------------------\n",
      "0.8375\n"
     ]
    }
   ],
   "source": [
    "num_trial_datapoints = 10000\n",
    "\n",
    "nn.train(X_train_flat[:num_trial_datapoints], \n",
    "         y_train[:num_trial_datapoints], \n",
    "         X_val_flat, y_val, \n",
    "         batch_size=64, \n",
    "         num_epochs=1000, \n",
    "         loss_type='cross_entropy', \n",
    "         log_every=5000)\n",
    "print('--'*20,'DONE','--'*20)\n",
    "print(nn.compute_accuracy(X_test_flat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8375"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.compute_accuracy(X_test_flat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129386128746312\n"
     ]
    }
   ],
   "source": [
    "t = 5\n",
    "a = 129386128746312\n",
    "# print a with t spaces\n",
    "print(f'{a :>{t}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
