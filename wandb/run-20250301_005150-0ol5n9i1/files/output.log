_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running NadamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch  1/10, Iteration    0/3375 --> Train Loss: 32.58068, Val Loss: 28.02523
Epoch  1/10, Iteration 1000/3375 --> Train Loss: 3.17098, Val Loss: 5.56745
Epoch  1/10, Iteration 2000/3375 --> Train Loss: 3.76289, Val Loss: 3.93196
Epoch  1/10, Iteration 3000/3375 --> Train Loss: 3.04686, Val Loss: 3.25790
Epoch  2/10, Iteration  625/3375 --> Train Loss: 2.44039, Val Loss: 2.71681
Epoch  2/10, Iteration 1625/3375 --> Train Loss: 2.70243, Val Loss: 2.36610
Epoch  2/10, Iteration 2625/3375 --> Train Loss: 1.47271, Val Loss: 2.08798
Epoch  3/10, Iteration  250/3375 --> Train Loss: 1.05128, Val Loss: 1.93587
Epoch  3/10, Iteration 1250/3375 --> Train Loss: 1.30814, Val Loss: 1.69332
Epoch  3/10, Iteration 2250/3375 --> Train Loss: 0.89060, Val Loss: 1.63436
Epoch  3/10, Iteration 3250/3375 --> Train Loss: 1.11882, Val Loss: 1.49133
Epoch  4/10, Iteration  875/3375 --> Train Loss: 1.67550, Val Loss: 1.42826
Epoch  4/10, Iteration 1875/3375 --> Train Loss: 0.95015, Val Loss: 1.34323
Epoch  4/10, Iteration 2875/3375 --> Train Loss: 2.21475, Val Loss: 1.26191
Epoch  5/10, Iteration  500/3375 --> Train Loss: 0.41549, Val Loss: 1.23932
Epoch  5/10, Iteration 1500/3375 --> Train Loss: 0.39176, Val Loss: 1.16570
Epoch  5/10, Iteration 2500/3375 --> Train Loss: 0.74881, Val Loss: 1.17671
Epoch  6/10, Iteration  125/3375 --> Train Loss: 0.27611, Val Loss: 1.05153
Epoch  6/10, Iteration 1125/3375 --> Train Loss: 0.54512, Val Loss: 1.08270
Epoch  6/10, Iteration 2125/3375 --> Train Loss: 0.56549, Val Loss: 0.98116
Epoch  6/10, Iteration 3125/3375 --> Train Loss: 0.83337, Val Loss: 0.93649
Epoch  7/10, Iteration  750/3375 --> Train Loss: 0.55457, Val Loss: 0.88496
Epoch  7/10, Iteration 1750/3375 --> Train Loss: 0.42656, Val Loss: 0.87663
Epoch  7/10, Iteration 2750/3375 --> Train Loss: 0.53228, Val Loss: 0.82494
Epoch  8/10, Iteration  375/3375 --> Train Loss: 0.29869, Val Loss: 0.85551
Epoch  8/10, Iteration 1375/3375 --> Train Loss: 1.27780, Val Loss: 0.89982
Epoch  8/10, Iteration 2375/3375 --> Train Loss: 0.34259, Val Loss: 0.83627
Epoch  9/10, Iteration    0/3375 --> Train Loss: 0.19111, Val Loss: 0.77968
Epoch  9/10, Iteration 1000/3375 --> Train Loss: 0.30440, Val Loss: 0.78353
Epoch  9/10, Iteration 2000/3375 --> Train Loss: 1.29602, Val Loss: 0.72561
Epoch  9/10, Iteration 3000/3375 --> Train Loss: 0.34719, Val Loss: 0.75843
Epoch 10/10, Iteration  625/3375 --> Train Loss: 1.64526, Val Loss: 0.75719
Epoch 10/10, Iteration 1625/3375 --> Train Loss: 0.40214, Val Loss: 0.72283
Epoch 10/10, Iteration 2625/3375 --> Train Loss: 0.35146, Val Loss: 0.75923
Test Accuracy: 0.8040
