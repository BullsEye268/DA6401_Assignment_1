_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running RMSpropOptimizer self.learning_rate = 0.0001 self.decay_rate = 0.9 self.epsilon = 1e-08
Epoch  1/10, Iteration    0/1688 --> Train Loss: 31.37945, Val Loss: 29.53541
Epoch  1/10, Iteration 1000/1688 --> Train Loss: 5.16255, Val Loss: 4.66169
Epoch  2/10, Iteration  312/1688 --> Train Loss: 2.04728, Val Loss: 3.19190
Epoch  2/10, Iteration 1312/1688 --> Train Loss: 2.35273, Val Loss: 2.56995
Epoch  3/10, Iteration  624/1688 --> Train Loss: 0.75680, Val Loss: 2.17620
Epoch  3/10, Iteration 1624/1688 --> Train Loss: 1.67365, Val Loss: 1.92888
Epoch  4/10, Iteration  936/1688 --> Train Loss: 1.31073, Val Loss: 1.63597
Epoch  5/10, Iteration  248/1688 --> Train Loss: 0.77614, Val Loss: 1.41782
Epoch  5/10, Iteration 1248/1688 --> Train Loss: 0.74059, Val Loss: 1.37959
Epoch  6/10, Iteration  560/1688 --> Train Loss: 1.18543, Val Loss: 1.24880
Epoch  6/10, Iteration 1560/1688 --> Train Loss: 0.99348, Val Loss: 1.17007
Epoch  7/10, Iteration  872/1688 --> Train Loss: 0.91547, Val Loss: 1.10526
Epoch  8/10, Iteration  184/1688 --> Train Loss: 0.89841, Val Loss: 1.04450
Epoch  8/10, Iteration 1184/1688 --> Train Loss: 0.50729, Val Loss: 1.00803
Epoch  9/10, Iteration  496/1688 --> Train Loss: 0.66727, Val Loss: 0.96887
Epoch  9/10, Iteration 1496/1688 --> Train Loss: 1.00207, Val Loss: 0.93175
Epoch 10/10, Iteration  808/1688 --> Train Loss: 0.64111, Val Loss: 0.87920
Test Accuracy: 0.7907
