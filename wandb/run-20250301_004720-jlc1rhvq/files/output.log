_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running NadamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch  1/10, Iteration    0/3375 --> Train Loss: 22.16997, Val Loss: 29.24147
Epoch  1/10, Iteration 1000/3375 --> Train Loss: 2.64204, Val Loss: 5.16880
Epoch  1/10, Iteration 2000/3375 --> Train Loss: 2.87053, Val Loss: 3.47266
Epoch  1/10, Iteration 3000/3375 --> Train Loss: 2.37529, Val Loss: 2.72057
Epoch  2/10, Iteration  625/3375 --> Train Loss: 1.98006, Val Loss: 2.24912
Epoch  2/10, Iteration 1625/3375 --> Train Loss: 0.57163, Val Loss: 1.94685
Epoch  2/10, Iteration 2625/3375 --> Train Loss: 1.11783, Val Loss: 1.78203
Epoch  3/10, Iteration  250/3375 --> Train Loss: 0.60600, Val Loss: 1.56118
Epoch  3/10, Iteration 1250/3375 --> Train Loss: 0.09351, Val Loss: 1.39553
Epoch  3/10, Iteration 2250/3375 --> Train Loss: 0.64622, Val Loss: 1.28580
Epoch  3/10, Iteration 3250/3375 --> Train Loss: 0.80190, Val Loss: 1.17983
Epoch  4/10, Iteration  875/3375 --> Train Loss: 0.51487, Val Loss: 1.10212
Epoch  4/10, Iteration 1875/3375 --> Train Loss: 0.96887, Val Loss: 1.03479
Epoch  4/10, Iteration 2875/3375 --> Train Loss: 1.15428, Val Loss: 0.97327
Epoch  5/10, Iteration  500/3375 --> Train Loss: 0.37073, Val Loss: 0.95286
Epoch  5/10, Iteration 1500/3375 --> Train Loss: 0.57269, Val Loss: 0.94347
Epoch  5/10, Iteration 2500/3375 --> Train Loss: 0.53964, Val Loss: 0.87114
Epoch  6/10, Iteration  125/3375 --> Train Loss: 0.15472, Val Loss: 0.83619
Epoch  6/10, Iteration 1125/3375 --> Train Loss: 0.43279, Val Loss: 0.83410
Epoch  6/10, Iteration 2125/3375 --> Train Loss: 0.39418, Val Loss: 0.83376
Epoch  6/10, Iteration 3125/3375 --> Train Loss: 0.35784, Val Loss: 0.75097
Epoch  7/10, Iteration  750/3375 --> Train Loss: 0.97682, Val Loss: 0.75865
Epoch  7/10, Iteration 1750/3375 --> Train Loss: 0.55502, Val Loss: 0.77950
Epoch  7/10, Iteration 2750/3375 --> Train Loss: 0.17862, Val Loss: 0.69934
Epoch  8/10, Iteration  375/3375 --> Train Loss: 0.45245, Val Loss: 0.68843
Epoch  8/10, Iteration 1375/3375 --> Train Loss: 0.22925, Val Loss: 0.70103
Epoch  8/10, Iteration 2375/3375 --> Train Loss: 0.24830, Val Loss: 0.67608
Epoch  9/10, Iteration    0/3375 --> Train Loss: 0.52667, Val Loss: 0.67831
Epoch  9/10, Iteration 1000/3375 --> Train Loss: 0.21274, Val Loss: 0.69304
Epoch  9/10, Iteration 2000/3375 --> Train Loss: 0.46858, Val Loss: 0.65381
Epoch  9/10, Iteration 3000/3375 --> Train Loss: 0.08637, Val Loss: 0.63397
Epoch 10/10, Iteration  625/3375 --> Train Loss: 0.21900, Val Loss: 0.64791
Epoch 10/10, Iteration 1625/3375 --> Train Loss: 0.29615, Val Loss: 0.65299
Epoch 10/10, Iteration 2625/3375 --> Train Loss: 0.46347, Val Loss: 0.63318
Test Accuracy: 0.8263
