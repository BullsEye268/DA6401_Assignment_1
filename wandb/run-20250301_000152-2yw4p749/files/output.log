_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running MomentumOptimizer self.learning_rate = 0.001 self.momentum = 0.9
Epoch  1/10, Iteration    0/1688 --> Train Loss: 26.06742, Val Loss: 23.05491
Epoch  1/10, Iteration 1000/1688 --> Train Loss: 0.56126, Val Loss: 0.72909
Epoch  2/10, Iteration  312/1688 --> Train Loss: 0.65918, Val Loss: 0.63684
Epoch  2/10, Iteration 1312/1688 --> Train Loss: 0.28052, Val Loss: 0.59171
Epoch  3/10, Iteration  624/1688 --> Train Loss: 0.59641, Val Loss: 0.57315
Epoch  3/10, Iteration 1624/1688 --> Train Loss: 0.62010, Val Loss: 0.54513
Epoch  4/10, Iteration  936/1688 --> Train Loss: 0.37483, Val Loss: 0.54912
Epoch  5/10, Iteration  248/1688 --> Train Loss: 0.51373, Val Loss: 0.52205
Epoch  5/10, Iteration 1248/1688 --> Train Loss: 0.37852, Val Loss: 0.51880
Epoch  6/10, Iteration  560/1688 --> Train Loss: 0.76778, Val Loss: 0.48513
Epoch  6/10, Iteration 1560/1688 --> Train Loss: 0.47450, Val Loss: 0.48535
Epoch  7/10, Iteration  872/1688 --> Train Loss: 0.55039, Val Loss: 0.47676
Epoch  8/10, Iteration  184/1688 --> Train Loss: 0.28455, Val Loss: 0.46873
Epoch  8/10, Iteration 1184/1688 --> Train Loss: 0.47941, Val Loss: 0.46373
Epoch  9/10, Iteration  496/1688 --> Train Loss: 0.26698, Val Loss: 0.46170
Epoch  9/10, Iteration 1496/1688 --> Train Loss: 0.35116, Val Loss: 0.44221
Epoch 10/10, Iteration  808/1688 --> Train Loss: 0.38772, Val Loss: 0.45387
Test Accuracy: 0.8376
