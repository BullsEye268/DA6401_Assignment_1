_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running NesterovOptimizer self.learning_rate = 0.0001 self.momentum = 0.9
Epoch  1/10, Iteration    0/1688 --> Train Loss: 2.28049, Val Loss: 2.29601
Epoch  1/10, Iteration 1000/1688 --> Train Loss: 2.07439, Val Loss: 1.96221
Epoch  2/10, Iteration  312/1688 --> Train Loss: 1.34508, Val Loss: 1.31835
Epoch  2/10, Iteration 1312/1688 --> Train Loss: 0.82215, Val Loss: 0.96790
Epoch  3/10, Iteration  624/1688 --> Train Loss: 0.84171, Val Loss: 0.83659
Epoch  3/10, Iteration 1624/1688 --> Train Loss: 0.79526, Val Loss: 0.76709
Epoch  4/10, Iteration  936/1688 --> Train Loss: 1.01381, Val Loss: 0.72256
Epoch  5/10, Iteration  248/1688 --> Train Loss: 0.92829, Val Loss: 0.68322
Epoch  5/10, Iteration 1248/1688 --> Train Loss: 0.71066, Val Loss: 0.65108
Epoch  6/10, Iteration  560/1688 --> Train Loss: 0.76162, Val Loss: 0.62174
Epoch  6/10, Iteration 1560/1688 --> Train Loss: 0.48714, Val Loss: 0.60026
Epoch  7/10, Iteration  872/1688 --> Train Loss: 0.67922, Val Loss: 0.59405
Epoch  8/10, Iteration  184/1688 --> Train Loss: 0.60106, Val Loss: 0.56697
Epoch  8/10, Iteration 1184/1688 --> Train Loss: 0.41533, Val Loss: 0.55707
Epoch  9/10, Iteration  496/1688 --> Train Loss: 0.47249, Val Loss: 0.53998
Epoch  9/10, Iteration 1496/1688 --> Train Loss: 0.51937, Val Loss: 0.54415
Epoch 10/10, Iteration  808/1688 --> Train Loss: 0.49413, Val Loss: 0.51982
Test Accuracy: 0.8132
