_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running AdamOptimizer self.learning_rate = 0.001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch 1/5, Iteration    0/3375 --> Train Loss: 3.28600, Val Loss: 2.89250
Epoch 1/5, Iteration 1000/3375 --> Train Loss: 0.77466, Val Loss: 0.74064
Epoch 1/5, Iteration 2000/3375 --> Train Loss: 0.91563, Val Loss: 0.58151
Epoch 1/5, Iteration 3000/3375 --> Train Loss: 0.72331, Val Loss: 0.53162
Epoch 2/5, Iteration  625/3375 --> Train Loss: 0.36143, Val Loss: 0.51522
Epoch 2/5, Iteration 1625/3375 --> Train Loss: 0.23110, Val Loss: 0.45636
Epoch 2/5, Iteration 2625/3375 --> Train Loss: 0.69158, Val Loss: 0.46257
Epoch 3/5, Iteration  250/3375 --> Train Loss: 0.31800, Val Loss: 0.44193
Epoch 3/5, Iteration 1250/3375 --> Train Loss: 0.25984, Val Loss: 0.45299
Epoch 3/5, Iteration 2250/3375 --> Train Loss: 0.35345, Val Loss: 0.44758
Epoch 3/5, Iteration 3250/3375 --> Train Loss: 0.80202, Val Loss: 0.41876
Epoch 4/5, Iteration  875/3375 --> Train Loss: 0.35763, Val Loss: 0.45094
Epoch 4/5, Iteration 1875/3375 --> Train Loss: 0.41367, Val Loss: 0.41279
Epoch 4/5, Iteration 2875/3375 --> Train Loss: 0.52278, Val Loss: 0.40422
Epoch 5/5, Iteration  500/3375 --> Train Loss: 0.33924, Val Loss: 0.45382
Epoch 5/5, Iteration 1500/3375 --> Train Loss: 0.81538, Val Loss: 0.42114
Epoch 5/5, Iteration 2500/3375 --> Train Loss: 0.37018, Val Loss: 0.39238
Test Accuracy: 0.8286
