_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running NesterovOptimizer self.learning_rate = 0.0001 self.momentum = 0.9
Epoch  1/10, Iteration    0/1688 --> Train Loss: 2.35495, Val Loss: 2.44783
Epoch  1/10, Iteration 1000/1688 --> Train Loss: 1.38362, Val Loss: 1.18017
Epoch  2/10, Iteration  312/1688 --> Train Loss: 1.00247, Val Loss: 0.90818
Epoch  2/10, Iteration 1312/1688 --> Train Loss: 0.59981, Val Loss: 0.77769
Epoch  3/10, Iteration  624/1688 --> Train Loss: 0.84458, Val Loss: 0.69934
Epoch  3/10, Iteration 1624/1688 --> Train Loss: 0.76916, Val Loss: 0.64751
Epoch  4/10, Iteration  936/1688 --> Train Loss: 0.72155, Val Loss: 0.60892
Epoch  5/10, Iteration  248/1688 --> Train Loss: 0.56651, Val Loss: 0.58231
Epoch  5/10, Iteration 1248/1688 --> Train Loss: 0.37728, Val Loss: 0.55810
Epoch  6/10, Iteration  560/1688 --> Train Loss: 0.33793, Val Loss: 0.54020
Epoch  6/10, Iteration 1560/1688 --> Train Loss: 0.54715, Val Loss: 0.52464
Epoch  7/10, Iteration  872/1688 --> Train Loss: 0.49551, Val Loss: 0.51342
Epoch  8/10, Iteration  184/1688 --> Train Loss: 0.63171, Val Loss: 0.50321
Epoch  8/10, Iteration 1184/1688 --> Train Loss: 0.34129, Val Loss: 0.49314
Epoch  9/10, Iteration  496/1688 --> Train Loss: 0.52185, Val Loss: 0.48586
Epoch  9/10, Iteration 1496/1688 --> Train Loss: 0.43368, Val Loss: 0.47996
Epoch 10/10, Iteration  808/1688 --> Train Loss: 0.40626, Val Loss: 0.47293
Test Accuracy: 0.8246
