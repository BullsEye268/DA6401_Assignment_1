_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running NadamOptimizer self.learning_rate = 0.001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch  1/10, Iteration    0/3375 --> Train Loss: 26.92142, Val Loss: 22.62321
Epoch  1/10, Iteration 1000/3375 --> Train Loss: 0.76193, Val Loss: 0.71076
Epoch  1/10, Iteration 2000/3375 --> Train Loss: 0.93010, Val Loss: 0.59886
Epoch  1/10, Iteration 3000/3375 --> Train Loss: 0.38045, Val Loss: 0.54848
Epoch  2/10, Iteration  625/3375 --> Train Loss: 0.56618, Val Loss: 0.50439
Epoch  2/10, Iteration 1625/3375 --> Train Loss: 1.16711, Val Loss: 0.47044
Epoch  2/10, Iteration 2625/3375 --> Train Loss: 0.35126, Val Loss: 0.48474
Epoch  3/10, Iteration  250/3375 --> Train Loss: 0.33829, Val Loss: 0.43839
Epoch  3/10, Iteration 1250/3375 --> Train Loss: 0.11262, Val Loss: 0.45336
Epoch  3/10, Iteration 2250/3375 --> Train Loss: 0.19947, Val Loss: 0.41650
Epoch  3/10, Iteration 3250/3375 --> Train Loss: 0.25274, Val Loss: 0.43441
Epoch  4/10, Iteration  875/3375 --> Train Loss: 0.14812, Val Loss: 0.41386
Epoch  4/10, Iteration 1875/3375 --> Train Loss: 0.17628, Val Loss: 0.39878
Epoch  4/10, Iteration 2875/3375 --> Train Loss: 0.48561, Val Loss: 0.38460
Epoch  5/10, Iteration  500/3375 --> Train Loss: 0.16918, Val Loss: 0.39077
Epoch  5/10, Iteration 1500/3375 --> Train Loss: 0.81339, Val Loss: 0.44579
Epoch  5/10, Iteration 2500/3375 --> Train Loss: 0.17475, Val Loss: 0.38698
Epoch  6/10, Iteration  125/3375 --> Train Loss: 0.18921, Val Loss: 0.37099
Epoch  6/10, Iteration 1125/3375 --> Train Loss: 0.68749, Val Loss: 0.37726
Epoch  6/10, Iteration 2125/3375 --> Train Loss: 0.08622, Val Loss: 0.36064
Epoch  6/10, Iteration 3125/3375 --> Train Loss: 0.35258, Val Loss: 0.36129
Epoch  7/10, Iteration  750/3375 --> Train Loss: 0.28177, Val Loss: 0.38347
Epoch  7/10, Iteration 1750/3375 --> Train Loss: 0.84687, Val Loss: 0.35305
Epoch  7/10, Iteration 2750/3375 --> Train Loss: 0.08992, Val Loss: 0.35591
Epoch  8/10, Iteration  375/3375 --> Train Loss: 0.45536, Val Loss: 0.35360
Epoch  8/10, Iteration 1375/3375 --> Train Loss: 0.17251, Val Loss: 0.38966
Epoch  8/10, Iteration 2375/3375 --> Train Loss: 0.19302, Val Loss: 0.37664
Epoch  9/10, Iteration    0/3375 --> Train Loss: 0.17097, Val Loss: 0.34935
Epoch  9/10, Iteration 1000/3375 --> Train Loss: 0.22045, Val Loss: 0.33944
Epoch  9/10, Iteration 2000/3375 --> Train Loss: 0.05766, Val Loss: 0.37550
Epoch  9/10, Iteration 3000/3375 --> Train Loss: 0.27187, Val Loss: 0.35967
Epoch 10/10, Iteration  625/3375 --> Train Loss: 0.43833, Val Loss: 0.34183
Epoch 10/10, Iteration 1625/3375 --> Train Loss: 0.17144, Val Loss: 0.35555
Epoch 10/10, Iteration 2625/3375 --> Train Loss: 0.11961, Val Loss: 0.35470
Test Accuracy: 0.8676
