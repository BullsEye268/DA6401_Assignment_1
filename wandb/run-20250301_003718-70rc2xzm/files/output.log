_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running NadamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch  1/10, Iteration    0/3375 --> Train Loss: 28.06276, Val Loss: 31.11965
Epoch  1/10, Iteration 1000/3375 --> Train Loss: 2.30876, Val Loss: 5.05746
Epoch  1/10, Iteration 2000/3375 --> Train Loss: 1.53948, Val Loss: 3.59239
Epoch  1/10, Iteration 3000/3375 --> Train Loss: 0.38537, Val Loss: 2.81081
Epoch  2/10, Iteration  625/3375 --> Train Loss: 1.26419, Val Loss: 2.39700
Epoch  2/10, Iteration 1625/3375 --> Train Loss: 1.66993, Val Loss: 2.07077
Epoch  2/10, Iteration 2625/3375 --> Train Loss: 1.07666, Val Loss: 1.76917
Epoch  3/10, Iteration  250/3375 --> Train Loss: 1.01069, Val Loss: 1.66969
Epoch  3/10, Iteration 1250/3375 --> Train Loss: 0.25654, Val Loss: 1.50303
Epoch  3/10, Iteration 2250/3375 --> Train Loss: 1.48569, Val Loss: 1.40401
Epoch  3/10, Iteration 3250/3375 --> Train Loss: 1.06198, Val Loss: 1.28857
Epoch  4/10, Iteration  875/3375 --> Train Loss: 0.58333, Val Loss: 1.20763
Epoch  4/10, Iteration 1875/3375 --> Train Loss: 0.68816, Val Loss: 1.10284
Epoch  4/10, Iteration 2875/3375 --> Train Loss: 1.70078, Val Loss: 1.09985
Epoch  5/10, Iteration  500/3375 --> Train Loss: 1.31184, Val Loss: 1.00093
Epoch  5/10, Iteration 1500/3375 --> Train Loss: 0.61923, Val Loss: 0.98393
Epoch  5/10, Iteration 2500/3375 --> Train Loss: 0.77808, Val Loss: 0.96888
Epoch  6/10, Iteration  125/3375 --> Train Loss: 1.06903, Val Loss: 0.89126
Epoch  6/10, Iteration 1125/3375 --> Train Loss: 0.34807, Val Loss: 0.85010
Epoch  6/10, Iteration 2125/3375 --> Train Loss: 0.29111, Val Loss: 0.81424
Epoch  6/10, Iteration 3125/3375 --> Train Loss: 0.25231, Val Loss: 0.83867
Epoch  7/10, Iteration  750/3375 --> Train Loss: 0.73702, Val Loss: 0.78191
Epoch  7/10, Iteration 1750/3375 --> Train Loss: 0.04968, Val Loss: 0.74600
Epoch  7/10, Iteration 2750/3375 --> Train Loss: 1.40342, Val Loss: 0.76759
Epoch  8/10, Iteration  375/3375 --> Train Loss: 1.40594, Val Loss: 0.71721
Epoch  8/10, Iteration 1375/3375 --> Train Loss: 0.18840, Val Loss: 0.72956
Epoch  8/10, Iteration 2375/3375 --> Train Loss: 0.19313, Val Loss: 0.71537
Epoch  9/10, Iteration    0/3375 --> Train Loss: 0.40942, Val Loss: 0.68974
Epoch  9/10, Iteration 1000/3375 --> Train Loss: 1.13736, Val Loss: 0.69057
Epoch  9/10, Iteration 2000/3375 --> Train Loss: 0.44261, Val Loss: 0.65028
Epoch  9/10, Iteration 3000/3375 --> Train Loss: 0.34089, Val Loss: 0.72591
Epoch 10/10, Iteration  625/3375 --> Train Loss: 0.36254, Val Loss: 0.64663
Epoch 10/10, Iteration 1625/3375 --> Train Loss: 0.19924, Val Loss: 0.65903
Epoch 10/10, Iteration 2625/3375 --> Train Loss: 0.57644, Val Loss: 0.64918
Test Accuracy: 0.8095
