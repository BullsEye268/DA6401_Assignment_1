_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running RMSpropOptimizer self.learning_rate = 0.001 self.decay_rate = 0.9 self.epsilon = 1e-08
Epoch 1/5, Iteration    0/3375 --> Train Loss: 2.39314, Val Loss: 2.58743
Epoch 1/5, Iteration 1000/3375 --> Train Loss: 0.67277, Val Loss: 0.62074
Epoch 1/5, Iteration 2000/3375 --> Train Loss: 0.21842, Val Loss: 0.54345
Epoch 1/5, Iteration 3000/3375 --> Train Loss: 1.07624, Val Loss: 0.48833
Epoch 2/5, Iteration  625/3375 --> Train Loss: 0.45895, Val Loss: 0.48638
Epoch 2/5, Iteration 1625/3375 --> Train Loss: 0.27661, Val Loss: 0.45597
Epoch 2/5, Iteration 2625/3375 --> Train Loss: 0.17993, Val Loss: 0.46014
Epoch 3/5, Iteration  250/3375 --> Train Loss: 0.46147, Val Loss: 0.45739
Epoch 3/5, Iteration 1250/3375 --> Train Loss: 0.65136, Val Loss: 0.42271
Epoch 3/5, Iteration 2250/3375 --> Train Loss: 0.45141, Val Loss: 0.42436
Epoch 3/5, Iteration 3250/3375 --> Train Loss: 0.10709, Val Loss: 0.40189
Epoch 4/5, Iteration  875/3375 --> Train Loss: 0.56997, Val Loss: 0.43237
Epoch 4/5, Iteration 1875/3375 --> Train Loss: 0.29844, Val Loss: 0.41698
Epoch 4/5, Iteration 2875/3375 --> Train Loss: 0.42205, Val Loss: 0.39460
Epoch 5/5, Iteration  500/3375 --> Train Loss: 0.09454, Val Loss: 0.40521
Epoch 5/5, Iteration 1500/3375 --> Train Loss: 0.22865, Val Loss: 0.38073
Epoch 5/5, Iteration 2500/3375 --> Train Loss: 0.42435, Val Loss: 0.38393
Test Accuracy: 0.8562
