_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running NadamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch  1/10, Iteration   0/844 --> Train Loss: 2.31284, Val Loss: 2.23148
Epoch  2/10, Iteration 156/844 --> Train Loss: 0.37590, Val Loss: 0.44885
Epoch  3/10, Iteration 312/844 --> Train Loss: 0.36296, Val Loss: 0.41206
Epoch  4/10, Iteration 468/844 --> Train Loss: 0.35486, Val Loss: 0.37636
Epoch  5/10, Iteration 624/844 --> Train Loss: 0.20407, Val Loss: 0.36883
Epoch  6/10, Iteration 780/844 --> Train Loss: 0.26779, Val Loss: 0.36086
Epoch  8/10, Iteration  92/844 --> Train Loss: 0.16149, Val Loss: 0.34457
Epoch  9/10, Iteration 248/844 --> Train Loss: 0.39275, Val Loss: 0.33828
Epoch 10/10, Iteration 404/844 --> Train Loss: 0.29531, Val Loss: 0.33653
Test Accuracy: 0.8695
