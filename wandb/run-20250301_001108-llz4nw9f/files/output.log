_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running NadamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch  1/10, Iteration    0/3375 --> Train Loss: 2.16169, Val Loss: 2.29263
Epoch  1/10, Iteration 1000/3375 --> Train Loss: 0.41766, Val Loss: 0.52242
Epoch  1/10, Iteration 2000/3375 --> Train Loss: 0.64960, Val Loss: 0.44748
Epoch  1/10, Iteration 3000/3375 --> Train Loss: 0.60525, Val Loss: 0.41791
Epoch  2/10, Iteration  625/3375 --> Train Loss: 0.42709, Val Loss: 0.40206
Epoch  2/10, Iteration 1625/3375 --> Train Loss: 0.34068, Val Loss: 0.39074
Epoch  2/10, Iteration 2625/3375 --> Train Loss: 0.28952, Val Loss: 0.38069
Epoch  3/10, Iteration  250/3375 --> Train Loss: 0.65333, Val Loss: 0.38291
Epoch  3/10, Iteration 1250/3375 --> Train Loss: 0.23766, Val Loss: 0.37071
Epoch  3/10, Iteration 2250/3375 --> Train Loss: 0.41190, Val Loss: 0.37473
Epoch  3/10, Iteration 3250/3375 --> Train Loss: 0.10186, Val Loss: 0.36079
Epoch  4/10, Iteration  875/3375 --> Train Loss: 0.48317, Val Loss: 0.35733
Epoch  4/10, Iteration 1875/3375 --> Train Loss: 0.21568, Val Loss: 0.36022
Epoch  4/10, Iteration 2875/3375 --> Train Loss: 0.51716, Val Loss: 0.35070
Epoch  5/10, Iteration  500/3375 --> Train Loss: 0.11329, Val Loss: 0.34093
Epoch  5/10, Iteration 1500/3375 --> Train Loss: 0.18180, Val Loss: 0.34632
Epoch  5/10, Iteration 2500/3375 --> Train Loss: 0.64078, Val Loss: 0.34783
Epoch  6/10, Iteration  125/3375 --> Train Loss: 0.26602, Val Loss: 0.34166
Epoch  6/10, Iteration 1125/3375 --> Train Loss: 0.58663, Val Loss: 0.33594
Epoch  6/10, Iteration 2125/3375 --> Train Loss: 0.11516, Val Loss: 0.33723
Epoch  6/10, Iteration 3125/3375 --> Train Loss: 0.42589, Val Loss: 0.33320
Epoch  7/10, Iteration  750/3375 --> Train Loss: 0.42682, Val Loss: 0.32020
Epoch  7/10, Iteration 1750/3375 --> Train Loss: 0.67691, Val Loss: 0.31813
Epoch  7/10, Iteration 2750/3375 --> Train Loss: 0.28501, Val Loss: 0.32670
Epoch  8/10, Iteration  375/3375 --> Train Loss: 0.13946, Val Loss: 0.31807
Epoch  8/10, Iteration 1375/3375 --> Train Loss: 0.51550, Val Loss: 0.32307
Epoch  8/10, Iteration 2375/3375 --> Train Loss: 0.20623, Val Loss: 0.33144
Epoch  9/10, Iteration    0/3375 --> Train Loss: 0.51166, Val Loss: 0.31304
Epoch  9/10, Iteration 1000/3375 --> Train Loss: 0.11478, Val Loss: 0.34963
Epoch  9/10, Iteration 2000/3375 --> Train Loss: 0.07280, Val Loss: 0.32321
Epoch  9/10, Iteration 3000/3375 --> Train Loss: 0.13948, Val Loss: 0.31299
Epoch 10/10, Iteration  625/3375 --> Train Loss: 0.19687, Val Loss: 0.31638
Epoch 10/10, Iteration 1625/3375 --> Train Loss: 0.73147, Val Loss: 0.32649
Epoch 10/10, Iteration 2625/3375 --> Train Loss: 0.14439, Val Loss: 0.31025
Test Accuracy: 0.8770
