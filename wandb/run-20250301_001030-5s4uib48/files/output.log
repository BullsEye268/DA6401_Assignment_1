_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running AdamOptimizer self.learning_rate = 0.001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch  1/10, Iteration    0/1688 --> Train Loss: 3.33639, Val Loss: 3.37116
Epoch  1/10, Iteration 1000/1688 --> Train Loss: 0.38666, Val Loss: 0.57534
Epoch  2/10, Iteration  312/1688 --> Train Loss: 0.63603, Val Loss: 0.49035
Epoch  2/10, Iteration 1312/1688 --> Train Loss: 0.26154, Val Loss: 0.47781
Epoch  3/10, Iteration  624/1688 --> Train Loss: 0.47408, Val Loss: 0.44412
Epoch  3/10, Iteration 1624/1688 --> Train Loss: 0.35398, Val Loss: 0.41988
Epoch  4/10, Iteration  936/1688 --> Train Loss: 0.23706, Val Loss: 0.41329
Epoch  5/10, Iteration  248/1688 --> Train Loss: 0.23095, Val Loss: 0.40392
Epoch  5/10, Iteration 1248/1688 --> Train Loss: 0.65614, Val Loss: 0.38305
Epoch  6/10, Iteration  560/1688 --> Train Loss: 0.33923, Val Loss: 0.38810
Epoch  6/10, Iteration 1560/1688 --> Train Loss: 0.38177, Val Loss: 0.39216
Epoch  7/10, Iteration  872/1688 --> Train Loss: 0.24490, Val Loss: 0.37858
Epoch  8/10, Iteration  184/1688 --> Train Loss: 0.28101, Val Loss: 0.37475
Epoch  8/10, Iteration 1184/1688 --> Train Loss: 0.25455, Val Loss: 0.38304
Epoch  9/10, Iteration  496/1688 --> Train Loss: 0.23155, Val Loss: 0.38917
Epoch  9/10, Iteration 1496/1688 --> Train Loss: 0.17019, Val Loss: 0.39246
Epoch 10/10, Iteration  808/1688 --> Train Loss: 0.40210, Val Loss: 0.37885
Test Accuracy: 0.8574
