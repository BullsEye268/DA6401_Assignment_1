_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running NadamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch  1/10, Iteration    0/3375 --> Train Loss: 30.22143, Val Loss: 31.07341
Epoch  1/10, Iteration 1000/3375 --> Train Loss: 3.23120, Val Loss: 2.79115
Epoch  1/10, Iteration 2000/3375 --> Train Loss: 1.96087, Val Loss: 1.33523
Epoch  1/10, Iteration 3000/3375 --> Train Loss: 0.93210, Val Loss: 0.98688
Epoch  2/10, Iteration  625/3375 --> Train Loss: 1.17219, Val Loss: 1.02371
Epoch  2/10, Iteration 1625/3375 --> Train Loss: 1.10575, Val Loss: 1.26592
Epoch  2/10, Iteration 2625/3375 --> Train Loss: 1.66504, Val Loss: 1.67428
Epoch  3/10, Iteration  250/3375 --> Train Loss: 2.04055, Val Loss: 2.06544
Epoch  3/10, Iteration 1250/3375 --> Train Loss: 2.30120, Val Loss: 2.28716
Epoch  3/10, Iteration 2250/3375 --> Train Loss: 2.32174, Val Loss: 2.30492
Epoch  3/10, Iteration 3250/3375 --> Train Loss: 2.30962, Val Loss: 2.30419
Epoch  4/10, Iteration  875/3375 --> Train Loss: 2.31558, Val Loss: 2.30383
Epoch  4/10, Iteration 1875/3375 --> Train Loss: 2.32539, Val Loss: 2.30359
Epoch  4/10, Iteration 2875/3375 --> Train Loss: 2.30516, Val Loss: 2.30328
Epoch  5/10, Iteration  500/3375 --> Train Loss: 2.28719, Val Loss: 2.30310
Epoch  5/10, Iteration 1500/3375 --> Train Loss: 2.30923, Val Loss: 2.30301
Epoch  5/10, Iteration 2500/3375 --> Train Loss: 2.30692, Val Loss: 2.30291
Epoch  6/10, Iteration  125/3375 --> Train Loss: 2.30499, Val Loss: 2.30283
Epoch  6/10, Iteration 1125/3375 --> Train Loss: 2.30809, Val Loss: 2.30281
Epoch  6/10, Iteration 2125/3375 --> Train Loss: 2.30892, Val Loss: 2.30282
Epoch  6/10, Iteration 3125/3375 --> Train Loss: 2.30305, Val Loss: 2.30274
Epoch  7/10, Iteration  750/3375 --> Train Loss: 2.29686, Val Loss: 2.30266
Epoch  7/10, Iteration 1750/3375 --> Train Loss: 2.30517, Val Loss: 2.30263
Epoch  7/10, Iteration 2750/3375 --> Train Loss: 2.30020, Val Loss: 2.30265
Epoch  8/10, Iteration  375/3375 --> Train Loss: 2.30546, Val Loss: 2.30264
Epoch  8/10, Iteration 1375/3375 --> Train Loss: 2.30534, Val Loss: 2.30265
Epoch  8/10, Iteration 2375/3375 --> Train Loss: 2.30309, Val Loss: 2.30263
Epoch  9/10, Iteration    0/3375 --> Train Loss: 2.29960, Val Loss: 2.30262
Epoch  9/10, Iteration 1000/3375 --> Train Loss: 2.30420, Val Loss: 2.30259
Epoch  9/10, Iteration 2000/3375 --> Train Loss: 2.30245, Val Loss: 2.30260
Epoch  9/10, Iteration 3000/3375 --> Train Loss: 2.30401, Val Loss: 2.30265
Epoch 10/10, Iteration  625/3375 --> Train Loss: 2.30155, Val Loss: 2.30264
Epoch 10/10, Iteration 1625/3375 --> Train Loss: 2.30298, Val Loss: 2.30264
Epoch 10/10, Iteration 2625/3375 --> Train Loss: 2.30292, Val Loss: 2.30263
Test Accuracy: 0.1000
