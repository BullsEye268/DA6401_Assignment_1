_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running AdamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch 1/5, Iteration    0/3375 --> Train Loss: 13.57285, Val Loss: 11.51841
Epoch 1/5, Iteration 1000/3375 --> Train Loss: 1.77289, Val Loss: 1.89464
Epoch 1/5, Iteration 2000/3375 --> Train Loss: 1.82183, Val Loss: 1.67952
Epoch 1/5, Iteration 3000/3375 --> Train Loss: 1.89722, Val Loss: 1.78247
Epoch 2/5, Iteration  625/3375 --> Train Loss: 1.89446, Val Loss: 1.98703
Epoch 2/5, Iteration 1625/3375 --> Train Loss: 2.19870, Val Loss: 2.19154
Epoch 2/5, Iteration 2625/3375 --> Train Loss: 2.26395, Val Loss: 2.29386
Epoch 3/5, Iteration  250/3375 --> Train Loss: 2.29913, Val Loss: 2.30523
Epoch 3/5, Iteration 1250/3375 --> Train Loss: 2.30097, Val Loss: 2.30478
Epoch 3/5, Iteration 2250/3375 --> Train Loss: 2.29633, Val Loss: 2.30428
Epoch 3/5, Iteration 3250/3375 --> Train Loss: 2.29381, Val Loss: 2.30395
Epoch 4/5, Iteration  875/3375 --> Train Loss: 2.30501, Val Loss: 2.30358
Epoch 4/5, Iteration 1875/3375 --> Train Loss: 2.30130, Val Loss: 2.30346
Epoch 4/5, Iteration 2875/3375 --> Train Loss: 2.28950, Val Loss: 2.30322
Epoch 5/5, Iteration  500/3375 --> Train Loss: 2.31333, Val Loss: 2.30316
Epoch 5/5, Iteration 1500/3375 --> Train Loss: 2.29668, Val Loss: 2.30300
Epoch 5/5, Iteration 2500/3375 --> Train Loss: 2.31356, Val Loss: 2.30291
Test Accuracy: 0.1000
