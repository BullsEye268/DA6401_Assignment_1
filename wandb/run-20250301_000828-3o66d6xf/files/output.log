_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running AdamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch  1/10, Iteration    0/1688 --> Train Loss: 2.35928, Val Loss: 2.35802
Epoch  1/10, Iteration 1000/1688 --> Train Loss: 0.49339, Val Loss: 0.49150
Epoch  2/10, Iteration  312/1688 --> Train Loss: 0.48314, Val Loss: 0.42861
Epoch  2/10, Iteration 1312/1688 --> Train Loss: 0.31897, Val Loss: 0.43657
Epoch  3/10, Iteration  624/1688 --> Train Loss: 0.49386, Val Loss: 0.40016
Epoch  3/10, Iteration 1624/1688 --> Train Loss: 0.29104, Val Loss: 0.41686
Epoch  4/10, Iteration  936/1688 --> Train Loss: 0.21750, Val Loss: 0.36220
Epoch  5/10, Iteration  248/1688 --> Train Loss: 0.52337, Val Loss: 0.35597
Epoch  5/10, Iteration 1248/1688 --> Train Loss: 0.30816, Val Loss: 0.35689
Epoch  6/10, Iteration  560/1688 --> Train Loss: 0.40239, Val Loss: 0.37113
Epoch  6/10, Iteration 1560/1688 --> Train Loss: 0.32174, Val Loss: 0.35001
Epoch  7/10, Iteration  872/1688 --> Train Loss: 0.27737, Val Loss: 0.34887
Epoch  8/10, Iteration  184/1688 --> Train Loss: 0.41618, Val Loss: 0.35554
Epoch  8/10, Iteration 1184/1688 --> Train Loss: 0.43041, Val Loss: 0.33113
Epoch  9/10, Iteration  496/1688 --> Train Loss: 0.23953, Val Loss: 0.34154
Epoch  9/10, Iteration 1496/1688 --> Train Loss: 0.39123, Val Loss: 0.32633
Epoch 10/10, Iteration  808/1688 --> Train Loss: 0.48404, Val Loss: 0.33704
Test Accuracy: 0.8680
