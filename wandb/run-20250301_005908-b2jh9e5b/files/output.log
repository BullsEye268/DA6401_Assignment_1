_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running NadamOptimizer self.learning_rate = 0.0001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch  1/10, Iteration    0/3375 --> Train Loss: 2.47994, Val Loss: 2.37498
Epoch  1/10, Iteration 1000/3375 --> Train Loss: 0.60806, Val Loss: 0.65192
Epoch  1/10, Iteration 2000/3375 --> Train Loss: 0.23637, Val Loss: 0.52415
Epoch  1/10, Iteration 3000/3375 --> Train Loss: 0.29855, Val Loss: 0.47072
Epoch  2/10, Iteration  625/3375 --> Train Loss: 0.75663, Val Loss: 0.44847
Epoch  2/10, Iteration 1625/3375 --> Train Loss: 0.38579, Val Loss: 0.42918
Epoch  2/10, Iteration 2625/3375 --> Train Loss: 0.30372, Val Loss: 0.42083
Epoch  3/10, Iteration  250/3375 --> Train Loss: 0.21364, Val Loss: 0.40486
Epoch  3/10, Iteration 1250/3375 --> Train Loss: 0.44597, Val Loss: 0.39416
Epoch  3/10, Iteration 2250/3375 --> Train Loss: 0.30768, Val Loss: 0.38563
