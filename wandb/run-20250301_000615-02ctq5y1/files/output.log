_wandb
activation
batch_size
epochs
hidden_layers
hidden_size
learning_rate
optimizer
weight_decay
weight_init
Running AdamOptimizer self.learning_rate = 0.001 self.beta1 = 0.9 self.beta2 = 0.999 self.epsilon = 1e-08
Epoch 1/5, Iteration    0/3375 --> Train Loss: 5.55720, Val Loss: 4.04346
Epoch 1/5, Iteration 1000/3375 --> Train Loss: 0.57290, Val Loss: 0.67357
Epoch 1/5, Iteration 2000/3375 --> Train Loss: 0.41477, Val Loss: 0.57993
Epoch 1/5, Iteration 3000/3375 --> Train Loss: 0.54844, Val Loss: 0.68044
Epoch 2/5, Iteration  625/3375 --> Train Loss: 0.16178, Val Loss: 0.51499
Epoch 2/5, Iteration 1625/3375 --> Train Loss: 0.94646, Val Loss: 0.56109
Epoch 2/5, Iteration 2625/3375 --> Train Loss: 0.36794, Val Loss: 0.49563
Epoch 3/5, Iteration  250/3375 --> Train Loss: 0.37039, Val Loss: 0.48474
Epoch 3/5, Iteration 1250/3375 --> Train Loss: 0.65702, Val Loss: 0.46471
Epoch 3/5, Iteration 2250/3375 --> Train Loss: 0.31559, Val Loss: 0.48379
Epoch 3/5, Iteration 3250/3375 --> Train Loss: 0.38737, Val Loss: 0.49053
Epoch 4/5, Iteration  875/3375 --> Train Loss: 0.35112, Val Loss: 0.44337
Epoch 4/5, Iteration 1875/3375 --> Train Loss: 0.34001, Val Loss: 0.46209
Epoch 4/5, Iteration 2875/3375 --> Train Loss: 0.62982, Val Loss: 0.46251
Epoch 5/5, Iteration  500/3375 --> Train Loss: 0.76502, Val Loss: 0.43061
Epoch 5/5, Iteration 1500/3375 --> Train Loss: 0.66709, Val Loss: 0.55032
Epoch 5/5, Iteration 2500/3375 --> Train Loss: 0.38514, Val Loss: 0.41727
Test Accuracy: 0.8351
